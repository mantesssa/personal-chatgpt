Making Higher Order MOT Scalable: An Efficient Approximate Solver for
Lifted Disjoint Paths
Andrea Hornakova‚àó1

arXiv:2108.10606v1 [cs.CV] 24 Aug 2021

‚àó

Timo Kaiser‚àó2
Bodo Rosenhahn2

Paul Swoboda1
Roberto Henschel2

Michal Rolinek3

Authors contributed equally, 1 Max Planck Institute for Informatics, Saarland Informatics Campus, 2 Institute for Information Processing,
Leibniz University Hannover, 3 Max Planck Institute for Intelligent Systems, TuÃàbingen

Abstract

ambiguous, especially in crowded scenes and pairwise costs
can be misleading. Moreover, object detectors produce
more errors in crowded scenes due to partial occlusions. To
resolve these issues, it is crucial that the data association
incorporates global context.
The disjoint paths problem (DP) [65, 35] is a natural
model for MOT. Results are computed efficiently using a
min-cost flow algorithm that delivers the global optimal solution. Unfortunately, the integration of long range temporal interactions is limited, as DP obeys the first-order
Markov-chain assumption: for each trajectory, consistency
is ensured only between directly linked detections, which is
a strong simplification that ignores higher order consistencies among multiple linked detections.
To fix this deficiency, [28] generalizes DP to lifted disjoint paths (LDP) by using additional connectivity priors
in terms of lifted edges. This makes the formulation much
more expressive while it maintains the feasibility set of the
DP (Sec. 3). The optimization problem enables to take into
account pairwise costs between arbitrary detections belonging to one trajectory. It thus enables to incorporate long
range temporal interactions effectively and leads to considerable improvement of recall and precision [28]. Similar extensions have been made for the multicut problem [56, 57].
While the integration of the global context by LDP is
crucial to obtain high-quality tracking results, it makes the
data association problem NP-hard. Still, [28] presented
a global optimal LDP solver usable for semi-crowded sequences with reasonable computational effort. However,
when applied to longer and crowded sequences, such approaches are not tractable anymore, due to too high demands on runtime and memory.
In order to close this gap, we present the first approximate solver for LDP. The resulting tracker scales to big
problem instances and incorporates global context with similar accuracy as the global optimal LDP solver. Moreover,
our solver outputs certificates in terms of primal/dual gaps.
In particular, our solver is based on a Lagrangean (dual)
decomposition of the problem. This dual is iteratively opti-

We present an efficient approximate message passing
solver for the lifted disjoint paths problem (LDP), a natural
but NP-hard model for multiple object tracking (MOT). Our
tracker scales to very large instances that come from long
and crowded MOT sequences. Our approximate solver enables us to process the MOT15/16/17 benchmarks without
sacrificing solution quality and allows for solving MOT20,
which has been out of reach up to now for LDP solvers due
to its size and complexity. On all these four standard MOT
benchmarks we achieve performance comparable or better than current state-of-the-art methods including a tracker
based on an optimal LDP solver.

1. Introduction
Deriving high-level understanding from a video is a desired task that has been studied in computer vision for a long
time. Nevertheless, solving the problem is a long way off.
A computer vision system able to extract the motions of objects appearing in a video in terms of trajectories is considered as a prerequisite for the goal. This task, called multiple
object tracking (MOT), has numerous applications, e.g. in
the area of video surveillance [18], sports analysis [2, 42],
urban planning [3], or autonomous driving [39, 19].
Yet, solving MOT is challenging, especially for long and
crowded sequences. The predominant approach for MOT is
the tracking-by-detection paradigm, which splits the problem into two subtasks. First, objects are detected in all video
frames by an object detector. Then, the detections are linked
across frames to form trajectories. While the performance
of object detectors has improved considerably by recent advances of CNNs [47, 63, 46, 17], the latter task called the
data association remains challenging. The data association
reasons from pairwise costs, which indicate for each pair of
detections the likelihood of belonging to the same object.
Appearance and spatio-temporal information are often
1

mized by dual block coordinate ascent (a.k.a. message passing) using techniques from [54], see Sec. 4.1. The decomposition relies on subproblems that are added in a cutting
plane fashion. We obtain high-quality primal solutions by
solving minimum cost flow problems with edge costs synthesizing information from both base and lifted edges from
the dual task and improve them via a local search procedure.
We validate the quality of the solver on four standard
MOT benchmarks (Sec. 5). We achieve comparable or better performance w.r.t. the current state-of-the-art trackers including the tracker based on the optimal LDP solver [28] on
MOT15/16/17 [37, 45]. Furthermore, our proposed tracker
performs on par with state-of-the-art on the more challenging MOT20 dataset [16] which is composed of long and
crowded sequences. Lightweight features and a fast solver
are crucial to perform tracking on such massive sequences.
Our work thus extends the applicability of the successful
LDP formulation to a wider range of instances.
Contribution of this work is in summary as follows:
‚Ä¢ We make the LDP problem more accessible and applicable by introducing an approximate solver with better
scalability properties than the global optimal LDP solver,
while resulting in similar tracking performance, and being independent of Gurobi [22].
‚Ä¢ We present an MOT system that is scalable to challenging
sequences by using considerably less computationally demanding features than what is used in the state-of-the-art
tracker [28]. Our system incorporates higher order consistencies in a scalable way, i.e. it uses an approximate
solver and provides a gap to the optimum.
We make our LDP solver1 and our MOT pipeline2 available.

plexity of the provided solver remains exponential. Therefore, in order to extend LDP-based methods to highly dense
MOT problems as in MOT20 it is crucial to reduce the complexity of the used LDP solver because the number of feasible connections between detections increases dramatically.
Multicut and lifted multicut. LDP is similar to (lifted)
multicut [14, 29]. Multicut has been used for MOT in [26,
33, 36, 51, 55, 56], lifted multicut in [5, 57]. These trackers
solve the underlying combinatorial problem approximately
via heuristics without providing an estimation of the gap to
optimality. Our approach, delivers an approximate solution
together with a lower bound enabling to assess the quality
of the solution. Additionally, LDP provides a strictly better
relaxation than lifted multicut [28].
Other data association models for MOT. Several works
employ greedy heuristics to obtain tracking results [9, 68,
7]. Such strategies normally suffer from occlusions or ambiguous situations, causing trajectory errors. Others use bipartite matchings [31, 52, 69, 62, 61, 60] to assign new detections with already computed trajectories of the past optimally. Since no global context is incorporated, they are
prone to errors if individual edge costs are misleading.
Higher order MOT frameworks ensure consistencies
within all detections of a trajectory. This can be done greedily, by computing one trajectory at a time via a generalized
minimum clique problem [64], or globally using an extension to the maximum multi clique problem [15].
Several works employ continuous domain relaxation.
When MOT is formulated as a binary quadratic program
[23, 25, 58, 24], a modification of the Frank-Wolfe algorithm adapted to the non-convex case has been used [23].
Some approximations for binary linear programs use an LPrelaxation, optimize in the continuous domain and derive
a binary solution from the continuous one [32, 12, 11]. They
however do not provide the optimality gap, in contrast to
our work. Higher order MOT can be considered as a classification problem using graph convolutions [10]. It allows to
train features directly on the tracking task.
The multigraph-matching problem, a generalization of
the graph matching problem, has been used for MOT
[30]. Here, cycle consistency constraints of the multi-graph
matching ensures higher order consistencies of the trajectories. Message passing for higher order matching in MOT
has been used in [4] employing a variant of MPLP [20]. In
contrast to our formulation, [4] does not model occlusions
and does not allow for connectivity priors.
Probabilistic approaches to multiple-target tracking include multiple hypotheses tracking [34, 13], joint probabiblistic data association [48, 53] and others [53, 44].

2. Related Work
(Lifted) disjoint paths. The disjoint paths problem is a
natural model for multiple object tracking and is solvable
with fast combinatorial solvers [35]. It has been used for
the data association step of MOT in [6, 65]. Its extensions
have been used for fusing different object detectors [12] or
multi-camera MOT [27, 38]. Its main disadvantage is that it
does not allow to integrate long range information because
it evaluates only direct connections between object detections within a trajectory. The lifted disjoint paths problem
introduced in [28] enhances DP by introducing lifted edges
that enable to reward or penalize arbitrary connections between object detections. This incorporation of long range
information leads to a significant improvement of the tracking performance yielding state-of-the-art results on main
MOT benchmark but makes the problem NP-hard. The authors provide a globally optimal solver using Gurobi [22].
Despite a lot of efficient subroutines, the general time com-

3. Problem Formulation
The lifted disjoint paths problem (LDP) introduced
in [28] is an optimization problem for finding a set of

1 https://github.com/LPMP/LPMP
2 https://github.com/TimoK93/ApLift

2

4. Lagrange Decomposition Algorithm for
LDP

vertex-disjoint paths in a directed acyclic graph. The cost
of each path is determined by the cost of edges in that path
as in the basic disjoint paths problem (DP), but additionally
there are higher order costs defined by lifted edges. A lifted
edge contributes to the cost if its endpoints are part of the
same path. This problem is a natural formulation for multiple object tracking (MOT), where lifted edges allow to reidentify the same objects over long distance.
While of greater expressivity, the LDP is NP-hard [28] in
contrast to DP which is reducible to the minimum cost flow.
Below, we recapitulate the formulation of LDP from [28].

Below we recapitulate Lagrange decomposition and the
message passing primitive used in our algorithm (Sec. 4.1).
Then, we propose a decomposition of the LDP problem (3)
into smaller but tractable subproblems (Sec. 4.2-4.4). This
decomposition is a dual task to an LP-relaxation of (3).
Therefore, it provides a lower bound that is iteratively increased by the message passing. We solve Problem (3) in
a simplified version of Lagrange decomposition framework
developed in [54]. Our heuristic for obtaining primal solutions uses the dual costs from the subproblems (Sec. 4.6).

3.1. Notation and Definitions.

4.1. Lagrange Decomposition

Flow network: a directed acyclic graph G = (V, E).
Start and terminal: nodes s, t ‚àà V .
Lifted graph: a directed acyclic graph G0 = (V 0 , E 0 ),
where V 0 = V \{s, t}.
The set of paths starting at v and ending in w is


vi vi+1 ‚àà E,
vw-paths(G) = (v1 v2 , . . . , vl‚àí1 vl ) :
.
v1 = v, vl = w
(1)
For a vw-path P its edge set is PE and its node set is PV .
Reachability relation for two nodes v, w ‚àà V is defined
as vw ‚àà RG ‚áî vw-paths(G) 6= ‚àÖ. We assume that it is
reflexive and su ‚àà RG , ut ‚àà RG ‚àÄu ‚àà V , i.e. all nodes can
be reached from s and all nodes can reach the sink node t.
Flow variables: Variables y ‚àà {0, 1}E have value 1 if flow
passes through the respective edges.
Node variables z ‚àà {0, 1}V denote flow passing through
each node. Values 0/1 forces paths to be node-disjoint.
Variables of the lifted edges E 0 are denoted by y 0 ‚àà
0
0
= 1 signifies that nodes v and w are con{0, 1}E . yvw
nected via the flow y in G. Formally,

We have an optimization problem minx‚ààX hc, xi where
X ‚äÜ {0, 1}n is a feasible set and c ‚àà Rn is the objective vector. Its Lagrange decomposition is given by a set of
subproblems S with associated feasible sets X s ‚äÜ {0, 1}ds
for each s ‚àà S. Each coordinate i of X s corresponds to
one coordinate of X via an injection œÄs : [ds ] ‚Üí [n] alternatively represented by a matrix As ‚àà {0, 1}ds,n where
(As )ij = 1 ‚áî œÄs (i) = j. For each pair of subproblems
s, s0 ‚àà S that contain a pair of coordinates i, j such that
0
œÄs (i) = œÄs0 (j), we have a coupling constraint xsi = xsj for
0
0
each xs ‚àà X s , xs ‚àà X s .
We require that every feasible solution x ‚àà X is feasible
for the subproblems, i.e. ‚àÄx ‚àà X , ‚àÄs ‚àà S : As x ‚àà X s .
We require that the objectives of subproblems
P are equivalent to the original objective, i.e. hc, xi = s‚ààS hŒ∏s , As xi
‚àÄx ‚àà X . Here, Œ∏s ‚àà Rds defines the objective of subproblem s.
The lower bound of the Lagrange decomposition given
the costs Œ∏s for each s ‚àà S is
X
hŒ∏s , xs i .
(4)
min
s
s

0
yvw
= 1 ‚áî ‚àÉP ‚àà vw-paths(G) : ‚àÄij ‚àà PE : yij = 1 .
(2)

s‚ààS

Lifted disjoint paths problem. Given edge costs c ‚àà RE ,
node cost d ‚àà RV in flow network G and edge cost c0 ‚àà
0
RE for the lifted graph G0 the lifted disjoint paths problem
is
min

0

0

Given coupling constraint xsi = xsj and Œ≥ ‚àà R, a se0
quence of operations of the form Œ∏is += Œ≥, Œ∏js ‚àí= Œ≥ is
called a reparametrization.
Feasible primal solutions are invariant under
reparametrizations but the lower bound (4) is not. The
optimum of the dual lower bound equals to the optimum of
a convex relaxation of the original problem, see [21].
Min-marginal message passing. Below, we describe
reparametrization updates monotonically non-decreasing in
the lower bound based on min-marginals. Given a variable
xsi of a subproblem s ‚àà S, the associated min-marginal is

hc, yi + hc0 , y 0 i + hd, zi

y‚àà{0,1}E ,y 0 ‚àà{0,1}E ,
z‚àà{0,1}V

s.t.

x ‚ààX

y node-disjoint s, t-flow in G,
z flow through nodes of G
y, y 0 feasible according to (2)

(3)
Set E 0 can be arbitrary. It makes sense to create a lifted
edge vw only if vw ‚àà RG due to Formula (2) and only if
v and w do not belong to neighboring frames. We describe
our choice in Sec. 5.2.
Other notation and abbreviations are in Appendix 8.1.

msi =

min

hŒ∏s , xs i ‚àí

xs ‚ààX s :xsi =1

min

xs ‚ààX s :xsi =0

hŒ∏s , xs i

(5)

i.e. the difference between the optimal solutions with the
chosen variable set to 1 resp. 0.
3

out

Œ∏is ‚àí= œâ ¬∑ msi ,

0

Œ∏js += œâ ¬∑ msi .

(6)

The goal of reparametrization is two-fold. (i) Improving
the objective lower bound to know how far our solution is
from the optimum. (ii) Using reparametrized costs as the
input for our primal heuristic yields high-quality primal solutions. The key components are efficient computations of
(i) optima of subproblems for obtaining lower bound (4),
(ii) constrained optima for obtaining min-marginals (5)
and (iii) a primal heuristic using the reparametrized costs
(Sec. 4.6). Lagrange decomposition has been used for other
problems but the subproblem decomposition and minimization procedures are problem specific. Therefore, developing
them for LDP is an important contribution for solving LDP
in a scalable way while keeping a small gap to an optimum.

E

Algorithm 1 Opt-Out-Cost
Input start vertex v, edge costs Œ∏ÃÉ
+
Output optimal value opt, lifted cost ‚àÄw : vw ‚àà Œ¥E
(v)
optimal solution for vw active Œ±vw
1: for u ‚àà V : vu ‚àà RG do
2:
lifted cost[u] = ‚àû, next[u] = ‚àÖ
3: end for
4: lifted cost[t] = 0, next[t] = t
5: Lifted-Cost-DFS-Out(v, v, Œ∏ÃÉ, lifted cost, next)
+
6: ‚àÄw : vw ‚àà Œ¥E (v) : Œ±vw = Œ∏ÃÉv + Œ∏ÃÉvw + lifted cost[w]
7: opt = min(minvw‚ààŒ¥ + (v) Œ±vw , 0)

4.2. Inflow and Outflow Subproblems
For each node v ‚àà V of the flow graph, we introduce two subproblems: An inflow and an outflow subproblem. The subproblems contain all incoming resp. outgoing
edges of node v together with the corresponding node. Formally, inflow resp. outflow subproblems contain the edges
‚àí
‚àí
+
+
Œ¥E
(v) ‚à™ Œ¥E
0 (v), resp. Œ¥E (w) ‚à™ Œ¥E 0 (w) . Here, we adopt
‚àí
+
the standard notation where Œ¥E (v), resp. Œ¥E
(v) denote all
base edges incoming to v, resp. outgoing from v. Simi‚àí
+
larly, Œ¥E
0 (v), Œ¥E 0 (v) denote lifted edges incoming to, resp.
outgoing from v.
The feasible set Xvout of the outflow subproblem for node
v is defined as
Ô£±
+
+
Ô£¥
zvout ‚àà {0, 1}, y out ‚àà {0, 1}Œ¥E (v) , y 0out ‚àà {0, 1}Œ¥E0 (v) :
Ô£¥
Ô£¥
Ô£¥
Ô£≤ (zvout , y out , y 0out ) = 0 ‚à®
‚àÉP ‚àà vt-paths(G) s.t.
zvout = 1
Ô£¥
out
Ô£¥
Ô£¥
yvw = 1 ‚áî vw ‚àà PE
Ô£¥
Ô£≥
0out
yvu
= 1 ‚áî u ‚àà PV
(7)
Consequently, either there is no flow going through vertex v
and all base and lifted edges have label zero. Alternatively,
there exists a vt-path P in G labeled by one. In this case
the base edge adjacent to v corresponding to the first edge
in P is one. All lifted edges connecting v with vertices
of P also have value one. All other base and lifted edges
are zero. Each feasible solution of the outflow subproblem
can be represented by a path vt-path P . The feasible set
of the inflow subproblem Xvin is defined analogously. We
sometimes omit the superscipts out for better readability.
Constraints between inflow and outflow subproblems.
For node variables, we add the constraint zvin = zvout . For
an edge vw ‚àà E ‚à™ E 0 we require the shared edge in the
outflow subproblem of v and in the inflow subproblem for

in

out
in
w to agree, i.e. yvw
= yvw
if vw ‚àà E and y 0 vw = y 0 vw if
0
vw ‚àà E .
Optimization of in- and outflow subproblems. Given
costs Œ∏out , the optimal solution of an outflow problem for
node v can be computed by depth-first search on the subgraph defined by the vertices reachable from v.
The algorithms rely on the following data structures:
‚Ä¢ lifted costs[u] contains the minimum cost of all ut-paths
w.r.t. to costs of all lifted edges connecting v with the
vertices of the path.
‚Ä¢ next[u] contains the best neighbor of vertex u
That is, next[u] =
w.r.t. values in lifted cost.
argminw:uw‚ààŒ¥+ (u) lifted cost[w]

Proposition 1 ([54]). Given a coupling constraints xsi =
0
xsj and œâ ‚àà [0, 1] the following operation is non-decreasing
w.r.t. the dual lower bound (4)

E

Algorithm 2 Lifted-Cost-DFS-Out

Ô£º
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£æ

Input v, u, Œ∏ÃÉ, lifted cost, next
Output lifted cost, next
1: Œ± = 0
+
2: for uw ‚àà Œ¥E (u) do
3:
if next[w] = ‚àÖ then Lifted-Cost-DFS-Out(v, w, Œ∏ÃÉ)
4:
if lifted cost[w] < Œ± then
Œ± = lifted cost[w], next[u] = w
. 5:
6:
end if
7: end for
8: if next[u] = ‚àÖ then next[u] = t
0
9: lifted cost[u] = Œ± + Œ∏ÃÉvu
Alg. 1 and 2 give a general dept first search (DFS) procedure that, given a vertex v, computes optimal paths from
all vertices reachable from v. Alg. 1 takes as input vertex v
and edge costs Œ∏ÃÉ. Its subroutine Alg. 2 computes recursively
for each vertex u reachable from v the value lifted cost[u].
The overall optimal cost min(z,y,y0 )‚ààXvout hŒ∏ÃÉ, (z, y, y 0 )i of
the subproblem is given by the minimum of node and base
out
edge and lifted edges costs minvu‚ààŒ¥+ (v) Œ∏ÃÉvout + Œ∏ÃÉvu
+
E
lifted cost[u]. We achieve linear complexity by exploiting
that subpaths of minimum cost paths are minimal as well.
The optimization for the inflow subproblem is analogous.
Message passing for in- and outflow subproblems. We

4

could compute one min-marginal (5) by adapting Alg. 1 and
forcing an edge to be taken or not. However, computing
min-marginals one-by-one with performing operation (6)
would be inefficient, since it would involve calling Alg. 1
+
+
O(|Œ¥E
(v)|+|Œ¥E
0 (v))| times. Therefore, we present efficient
algorithms for computing a sequence of min-marginals in
Appendix 8.2. The procedures save computations by choosing the order of edges for computing min-marginals suitably
and reuse previous calculations.

subproblems, we add during the optimization only those
that improve the relaxation. Details are in Appendix 8.4.

4.4. Cut Subproblems
The purpose of a cut subproblem is to reflect that a lifted
edge uv must be labelled 0 if there exists a cut of base edges
that separate u and v (uv-cut) all labelled 0.
The feasible set. A cut subproblem consists of a lifted edge
uv and a uv-cut C = {ij ‚àà E|i ‚àà A, j ‚àà B} where
A, B ‚äÇ V with A ‚à© B = ‚àÖ. The space of feasible solutions
X C is defined as
X
0
0
yuv
‚àà {0, 1}, y ‚àà {0, 1}C : yuv
‚â§
yij ,

4.3. Path Subproblems
The subproblem contains a lifted edge vw and a path P
from v to w consisting of both base and lifted edges. They
reflect that (i) lifted edge vw must be labelled 1 if there exists an active path between v and w, and (ii) there cannot
be exactly one inactive lifted edge within path P if vw is
active. The reason is that the inactive lifted edge divides P
into two segments that must be disconnected. This is contradictory to activating lifted edge vw because it indicates a
connection between v and w. Path subproblems are similar
to cycle inequalities for the multicut [14].
In order to distinguish between base and lifted edges of
path P , we use notation PE = P ‚à© E and PE 0 = P ‚à© E 0 .
For the purpose of defining the feasible solutions of path
subproblems, we define strong base edges E0 = {vw ‚àà
E|vw-paths(G) = {vw}}. That is, base edge vw is strong
iff there exists no other vw-path in graph G than vw itself.
The feasible set X P of the path subproblem for vw-path
P is defined as

ij‚ààC

‚àÄi ‚àà A :

0
uv ‚àà C ‚áí yuv
‚â• yuv .

ij‚ààPE

0
‚â• 1 ‚àí ykl
,

(9)
X

X

yij ‚â§ 1 ,

ij‚ààC

(10)

Algorithm 3 Cut-Subproblem-Optimization
Input Edge costs Œ∏C
Output optimal value opt of subproblem.
1: DefineÔ£±
œà ‚àà RA√óB :
C
0C
0C
Ô£¥Œ∏uv
+ Œ∏uv
, if ij = uv ‚àß uv ‚àà C ‚àß Œ∏uv
>0
Ô£≤
2: œàij = ‚àû,
if ij ‚àà
/C
Ô£¥
Ô£≥ C
Œ∏ij ,
otherwise
P P
3: z ‚àó ‚àà argmin
œàij zij , s.t. z 1 ‚â§ 1, z > 1 ‚â§ 1
z‚àà{0,1}A√óB i‚ààA j‚ààB
P
‚àó
4: opt =
ij‚ààC œàij zij
0C
5: if Œ∏uv ‚â• 0 then return opt
6: if ‚àÉkl ‚àà C : zkl = 1 then
0C
7:
return opt + Œ∏uv
8: else
C
9:
Œ± = minij‚ààC Œ∏ij
0C
0C
10:
if |Œ∏uv | > Œ± then return Œ∏uv
+Œ±
11:
else return opt
12: end if

ij‚ààPE 0 ‚à™{vw}\{kl}

‚àÄkl ‚àà PE ‚à© E0 :
X
(1 ‚àí yij ) +
ij‚ààPE \kl

(1 ‚àí

0
yij
)

‚àÄj ‚àà B :

The constraints stipulate that (i) the lifted edge uv is 0 if all
the edges in the cut are 0, (ii) there exists at most one active
outgoing resp. incoming edge for every vertex in A resp. B
and (iii) if there is also base edge uv ‚àà C then whenever it
is active, the lifted edge uv must be active.

(8)
X

yij ‚â§ 1 ,

ij‚ààC

y ‚àà{0, 1}PE , y 0 ‚àà {0, 1}PE0 ‚à™{vw} :
‚àÄkl ‚àà PE 0 ‚à™ {vw} :
X
(1 ‚àí yij ) +

X

(1 ‚àí yij ) ‚â• 1 ‚àí ykl .

ij‚ààPE 0 ‚à™{vw}

Equation (8) requires that a lifted edge in PE 0 or vw can
be zero only if at least one other edge of the subproblem is
zero. Equation (9) enforces the same for strong base edges.
The optimization of path subproblems is detailed in
Alg. 12 in the Appendix. The principle is as follows. It
checks whether there exists exactly one positive edge and
whether it is either a lifted or a strong base edge. If so,
the optimal solution is either (i) all edges except the two
largest ones or (ii) all edges, whichever gives smaller objective value. If the above condition does not hold, the optimal
solution can be chosen to contain all negative edges.
We use a variation of the path optimization algorithm
with an edge fixed to 0 or 1 for computing min-marginals.
Cutting plane. Since there are exponentially many path

Optimization of a cut subproblem with respect to feasible set X C is given by Alg. 3. Its key is to solve a linear
assignment problem (LAP) [1] between vertex sets A and
B. The assignment cost œàij for (i, j) ‚àà A √ó B is the cut
C
edge cost Œ∏ij
if edge ij belongs to C and ‚àû otherwise. In
the special case of uv-cut C containing base edge uv and
0C
being positive, the assignment cost
the lifted edge cost Œ∏uv
0C
œàuv is increased by Œ∏uv
.
A candidate optimal labeling of cut edges is given by
0C
values of LAP variables zij . If Œ∏uv
‚â• 0, the optimal value
5

found by the LAP is the optimal value of the cut subproblem. If it is negative, we distinguish two cases: (i) If a cut
0C
edge kl labeled by one exists, the lifted edge cost Œ∏uv
is
added to the optimal value of LAP. (ii) Otherwise, we inspect whether it is better to activate the smallest-cost cut
edge and the lifted edge uv or keep all edges inactive.
We use a variation of Alg. 3 with an edge variable restricted to be either 0 or 1 for computing min-marginals.
Cutting plane. There are exponentially many cut subproblems. Therefore, we add only those that improve the lower
bound. See Appendix 8.5 for details.

whether cutting off one node from the first path‚Äôs end or the
second paths‚Äôs beginning makes the connection possible. If
yes and the connection is decreasing the objective, the nodes
are cut off and the paths are connected.
Algorithm 4 Init-MCF
1: ‚àÄu ‚àà V \{s, t}:
(o, lc, Œ±in )=Opt-In-Cost(u, Œ∏uin )
(o, lc, Œ±out )=Opt-Out-Cost(u, Œ∏uout )
mcf
in
out
2: ‚àÄu ‚àà V \{s, t} : Œ∏suin = Œ±su
, Œ∏umcf
out t = Œ±ut
mcf
out
in
3: ‚àÄu ‚àà {uv ‚àà E|u 6= s, v 6= t} : Œ∏uout v in = Œ±uv
+ Œ±uv

4.5. Message Passing
The overall algorithm for optimizing the Lagrange decomposition is Alg. 19 in the Appendix. First, inflow and
outflow subproblems are initialized for every node. Then,
for a number of iterations or until convergence, costs for
each subproblems are adjusted iteratively by computing
min-marginals and adjusting the reparametrization proportionally to the min-marginal‚Äôs value. Additionally, every
k-th iteration additional path and cut subproblems are separated and added to the Lagrange decomposition.
Solver complexity. We need O(|E inp |) space where E inp
are all edges before graph sparsification. The most time
consuming is computing lifted edges min-marginals for
each in/outflow subproblem. Alg. 6 computes them for one
outflow subproblem and it is linear in the number of detections per frame. This significantly improves the complexity
of to the optimal LDP solver LifT, making LDP applicable
to large problem instances. See Appendix 8.14 for details.

5. Experiments
We integrate our LDP solver into an MOT system (Appendix, Fig. 1) and show on challenging datasets that higher
order MOT is scalable to big problem instances. In the next
sections, we describe our experimental setup and present results. We clarify the edge cost calculation and construction
of the base and the lifted graph and their sparsification.

5.1. Pairwise Costs
We use multi layer perceptrons (MLP) to predict the likelihood that two detections belong to the same trajectory. We
divide the maximal frame distance into 20 intervals of equal
length and train one separate MLP for each set of frame distances. We transform the MLP output to the cost of the edge
between the detections and use it in our objective (3). Negative cost indicates that two detections belong to the same
trajectory. Positive cost reflects the opposite.
MLP architecture. Each MLP consists of a fully connected layer with the same number of neurons as the input
size, followed by a LeakyReLU activation [43] and a fully
connected single neuron output layer. We add sigmoid activation in the training. We describe our spatial and visual
features used as the input in the paragraphs below.
Spatial feature uses bounding box information of two detections v and w. We align the boxes such that their centers overlap. The similarity feature œÉvw,Spa ‚àà [0, 1] is the
intersection-over-union between two aligned boxes.
Appearance feature. We create an appearance feature Fv
for each detection v by training the method [67] on the training set of the respective benchmark and additional data from
[66, 59, 50]. The similarity feature œÉvw,App between detection v and w given by œÉvw,App := max{0, hFv , Fw i} is
used. A higher value indicates a higher similarity.
Global context normalization. The two features œÉvw,Spa ,
œÉvw,App depend entirely on the nodes v and w. To include
global context, we append several normalized versions of
the two features to the edge feature vector, inspired by [28].
Both features œÉij,‚àó of edge ij undergo a five-way normalization. In each case, the maximum feature value from a rel-

4.6. Primal Rounding
For computing primal solutions we solve a minimum
cost flow (MCF) problem on the base edges and improve
this initial solution with a local search heuristic.
Without lifted edges, the disjoint paths problem is an instance of MCF, which can be efficiently optimized via combinatorial solvers like the successive shortest path solver
that we employ [1]. We enforce node disjoint paths via splitting each node u ‚àà V into two nodes uin , uout ‚àà V mcf in
the MCF graph Gmcf = (V mcf , E mcf ), adding an additional edge uin uout to E mcf and setting capacity [0, 1] on
all edges E mcf . Each node except s and t has demand 0.
Alg. 4 calculates MCF edge costs from in/outflow subproblems using Alg. 1. We obtain the cost of each flow edge
uout v in from the inflow subproblem of v and the outflow
subproblem of u using their minima where edge uv is active. This combines well the cost from base and lifted edges.
We describe the local search heuristic for improving the
MCF solution in Alg. 25 in the Appendix. It works with
sets of disjoint paths. First, paths are split if this leads to a
decrease in the objective. Second, merges are explored. If
a merge of two paths is not possible, we iteratively check
6

evant set of edges is selected as the normalization value.
The normalization is done by dividing the two features œÉij,‚àó
by each of their five normalization values. This yields 10
values. Another set of 10 values for edge ij is obtained by
2
by each of the five normalization values. Todividing œÉij,‚àó
gether with the two unnormalized features œÉij,‚àó , edge feature vectors have length 22. See Appendix 8.9 for details.
Training. We iteratively train our MLP on batches B containing sampled edges. To compensate the imbalance between true positive and true negative edges, we use an Œ±balanced focal loss [40] with Œ≥ = 1. We define the Œ±weight Œ±(g,‚àÜf ) to weight the correct classification of edge
vw with ground truth flow value gvw ‚àà {0, 1}, time distance ‚àÜf between v in frame fv and w in frame fw , and
value g ‚àà {0, 1} via Œ±(g,‚àÜf ) := 1/|{vw ‚àà E : |fv ‚àí fw | =
‚àÜf, gvw = g}| . We optimize the classifier using Adam
with lr = 0.1, Œ≤1 = 0.9, Œ≤2 = 0.999 and  = 10‚àí8 . To
reduce complexity while maintaining variety during training, we introduce an extended sampling. Given a frame f ,
we create batches B(f ) by sampling detections from a fixed
sequence of frame shifts starting at frame f ensuring that all
temporal distances ‚àÜf are present in B(f ) (details in Appendix 8.11). We then subsample the k-nearest detections
to a random generated image position with k = 160, which
sensitizes training to crowded scenes. We train the MLP for
3 epochs with batches B(f ) for all frames f of the dataset.

avoid double counting of edge costs, we subsequently set
costs of all base edges between non-consecutive frames to
zero, so that only lifted edges maintain the costs. If a lifted
edge has cost around zero, it is not discriminative and we
remove it, unless it overlaps with a (zero-valued) base edge.

5.3. Inference
For fair comparison to state of the art, we filter and refine
detections using tracktor [7] as in [28]. Different to [28], we
apply tracktor to recover missing detections before running
the solver.
While we solve MOT15/16/17 on global graphs, we
solve MOT20 in time intervals in order to decrease memory
consumption and runtime. First, we solve the problem on
non-overlapping adjacent intervals and fix the trajectories
in the interval centers. Second, we solve the problem on a
new set of intervals where each of them covers unassigned
detections in two initial neighboring intervals and enables
connections to the fixed trajectory fragments. We use the
maximal edge length of 50 frames in MOT20. Therefore,
150 is the minimal interval length such that all edges from
a detection are used when assigning the detection to a trajectory. This way, the solver has sufficient context for making
each decision. Intervals longer than 200 frames increase
the complexity significantly for MOT20, therefore we use
interval length 150 in our experiments.
Post-processing. We use simple heuristics to check if base
edges over long time gaps correspond to plausible motions,
and split trajectories if necessary. Finally, we use linear interpolation to recover missing detections within a trajectory.
Appendix 8.13 contains further details on inference.

5.2. Graph Construction
We create the base and the lifted graph edges between
detections with time distance up to 2 seconds. We also add
an edge from source s, and to sink t to each detection. In
order to reduce computational complexity, we apply sparsification on both base and lifted graph as described later.
Costs. We obtain base and lifted costs c and c0 from the
same MLP classifier (Sec. 5.1). Due to decreasing classification accuracy with increasing frame distance ‚àÜf , we
multiply the costs by a decay weight œâ‚àÜf := (10 ¬∑ ‚àÜf +
0.1)‚àí1 , so that edges representing long temporal distances
have lower weight. Edges from s and to t have costs zero.
Finally, we use simple heuristics to find pairs that are
obviously matching or non-matching. We set the corresponding costs to be high in absolute value, negative for
matching and positive for non-matching, thereby inducing
soft constrains. An obvious match is given by a nearly maximal feature similarity. Detection pairs are obviously nonmatching, if the displacement between their bounding boxes
is too high. See Appendix 8.12 for details.
Sparsification. The base edges are an intersection of two
edge sets. The first set contains for every v ‚àà V 0 edges to
its 3 nearest (lowest-cost) neighbors from every subsequent
time frame. The second set selects for every vertex the best
edges to its preceding frames analogically. Moreover, edges
longer than 6 frames must have costs lower than 3.0. To

5.4. Tracking Evaluation
We evaluate our method on four standard MOT benchmarks. The MOT15/16/17 benchmarks [37, 45] contain
semi-crowded videos sequences filmed from a static or
a moving camera. MOT20 [16] comprises crowded scenes
with considerably higher number of frames and detections
per frame, see Tab. 1. The challenge does not come only
with the data size. Detectors make more errors in crowded
scenes due to frequent occlusions and appearance features
are less discriminative as the distance of people to the camera is high. Using higher order information helps in this
context. However, the number of edges in our graphs grows
quadratically with the number of detections per frame.
Therefore, it is crucial to make the tracker scalable to these
massive data. We use the following ingredients to solve the
problems: (i) fast but accurate method for obtaining edge
costs, (ii) approximate LDP solver delivering high-quality
results fast, (iii) preprocessing heuristics, (iv) interval solution keeping sufficient context for each decision.
We use training data of the corresponding dataset for
training and the public detections for training and test.
7

MOT17

CTTrackPub [68]
ApLift (ours)
Lif T [28]
MPNTrack [10]

61.5
60.5
60.5
58.8

59.6
65.6
65.6
61.7

621
798
637
679

752
728
791
788

14076
30609
14966
17416

200672
190670
206619
213594

2583
1709
1189
1185

4965
2672
3476
2265

845.6

31.8

MOT16

ApLift (ours)
Lif T [28]
MPNTrack [10]
GSM [41]

61.7
61.3
58.6
57.0

66.1
64.7
61.7
55.0

260
205
207
167

237
258
258
262

9168
4844
4949
4332

60180
65401
70252
73573

495
389
354
475

802
1034
684
859

845.6

30.8

MOT15

MOT20

Table 1. Comparison of ApLift with the best performing solvers w.r.t. MOTA metric on the MOT challenge. ‚Üë higher is better, ‚Üì lower is
better. The two rightmost columns: average number of frames per sequence and the average number of detections per frame for dataset.
Method
MOTA‚Üë IDF1‚Üë MT‚Üë ML‚Üì
FP‚Üì
FN‚Üì
IDS‚Üì Frag‚Üì Frames Density
ApLift (ours)
58.9
56.5
513
264
17739
192736 2241
2112
MPNTrack [10]
57.6
59.1
474
279
16953
201384
1210 1420 1119.8
170.9
Tracktor++v2 [7]
52.6
52.7
365
331
6930
236680
1648
4374

Lif T [28]
MPNTrack [10]
ApLift (ours)
Tracktor15 [7]

52.5
51.5
51.1
44.1

60.0
58.6
59.0
46.7

244
225
284
130

186
187
163
189

6837
7260
10070
6477

21610
21780
19288
26577

730
375
677
1318

1047
872
1022
1790

525.7

10.8

Table 2. Influence of lifted graph sparsification, message passing
and using zero base costs on MOT17 train without postprocessing.
MP Base
E 0 steps cost IDF1‚Üë MOTA‚Üë FP‚Üì
FN‚Üì
IDS‚Üì
Dense
Dense
Dense
Sparse

82
0
82
82

Zero 71.0
Zero 70.3
Orig. 69.8
Orig. 69.1

66.3
66.3
66.3
66.3

2826
2832
2824
2825

109263
109265
109266
109263

slightly high FP values. FP/FN are mostly affected by preprocessing the input detections and interpolation in the postprocessing. The impact of post-processing (trajectory splits
and interpolations) on MOT20, which causes FP but reduces FN and IDS, is analyzed in the Appendix (Tab. 4).
Tab. 2 shows the influence of various settings on the performance of MOT17 train. While we usually set the base
edge costs to zero (Sec. 5.2), we need to keep them when
using the sparsified lifted graph. Both, message passing
and dense lifted edges improve IDF1 and IDS. However,
MOTA, FN and FP remain almost unchanged.
Finally, we compare the runtime of our solver against
the two step version of LifT for a sample sequence in
Tab. 3. With increasing problem complexity, our solver outperforms LifT w.r.t. runtime while achieving similar IDF1.
Counter-intuitively, as we progress towards increasingly
better optimization objective values, the tracking metrics
can slightly decrease due to imperfect edge costs. We compare our solver against optimal (one step) LifT on MOT17
train in Appendix 8.14.

1369
1354
1355
1316

Table 3. Runtime and IDF1 comparison of LDP solvers: ApLift
(ours) with 6, 11, 31 and 51 iterations and LifT[28] (two step procedure) on first n frames of sequence MOT20-01 from MOT20.
n
Measure LifT Our6 Our11 Our31 Our51
IDF1‚Üë
80.6 83.3
83.3
81.5
81.5
50
time [s]
272
2
4
16
35
100

IDF1‚Üë
time [s]

80.4
484

82.5
14

82.5
24

81.6
97

81.6
218

150

IDF1‚Üë
time [s]

78.1
1058

81.0
25

81.0
46

79.8
192

79.8
431

200

IDF1‚Üë
time [s]

73.2
2807

75.4
36

75.4
66

74.6
277

74.6
616

We compare our method using standard MOT metrics.
MOTA [8] and IDF1 [49] are considered the most representative as they incorporate other metrics (in particular recall
and precision). IDF1 is more penalized by inconsistent trajectories. We also report mostly tracked (MT) and mostly
lost trajectories (ML), false negatives (FN) and false positives (FP), ID switches (IDS) and fragmentations (Frag) as
provided by the evaluation protocols [8] of the benchmarks.
Tab. 1 shows the comparison to the best (w.r.t. MOTA)
peer-reviewed methods on test sets. Our approximate solver
achieves almost the same results on MOT15/16/17 as the
optimal LDP solver [28], while using simpler features.
Overall, our method performs on par with state of the art on
all evaluated benchmarks, especially in MOTA and IDF1.
Our complete results and videos are publicly available3 .
The proposed method achieves overall low FN values but

6. Conclusion
We demonstrated that the NP-hard LDP model is applicable for processing massive sequences of MOT20. The
combination of an approximate LDP solver, efficiently
computable costs and subdivision of data keeping sufficient
context for each decision make this possible.

7. Acknowledgements
This work was supported by the Federal Ministry of Education and Research (BMBF), Germany, under the project
LeibnizKILabor (grant no. 01DD20003), the Center for
Digital Innovations (ZDIN) and the Deutsche Forschungsgemeinschaft (DFG) under Germany‚Äôs Excellence Strategy
within the Cluster of Excellence PhoenixD (EXC 2122).

3 https://motchallenge.net/method/MOT=4031&chl=13

8

References

[15] Afshin Dehghan, Shayan Modiri Assari, and Mubarak Shah.
GMMCP tracker: Globally optimal generalized maximum
multi clique problem for multiple object tracking. In IEEE
Conference on Computer Vision and Pattern Recognition,
pages 4091‚Äì4099, 2015. 2
[16] Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen
Shi, Daniel Cremers, Ian Reid, Stephan Roth, Konrad Schindler, and Laura Leal-TaixeÃÅ.
Mot20: A
benchmark for multi object tracking in crowded scenes.
arXiv:2003.09003[cs], Mar. 2020. arXiv: 2003.09003. 2,
7, 24, 28, 30
[17] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, and Qi Tian. Centernet: Keypoint triplets for
object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6569‚Äì6578,
2019. 1
[18] Michele Fenzi, JoÃàrn Ostermann, Nico Mentzer, Guillermo
PayaÃÅ-VayaÃÅ, Holger Blume, Tu Ngoc Nguyen, and Thomas
Risse. Asev‚Äîautomatic situation assessment for eventdriven video analysis. In 2014 11th IEEE International Conference on Advanced Video and Signal Based Surveillance
(AVSS), pages 37‚Äì43. IEEE, 2014. 1
[19] Davi Frossard and Raquel Urtasun. End-to-end learning of
multi-sensor 3d tracking by detection. In 2018 IEEE international conference on robotics and automation (ICRA), pages
635‚Äì642. IEEE, 2018. 1
[20] Amir Globerson and Tommi Jaakkola.
Fixing maxproduct: Convergent message passing algorithms for map
lp-relaxations. Advances in neural information processing
systems, 20:553‚Äì560, 2007. 2
[21] Monique Guignard and Siwhan Kim.
Lagrangean
decomposition for integer programming: theory and
applications.
RAIRO-Operations Research-Recherche
OpeÃÅrationnelle, 21(4):307‚Äì323, 1987. 3
[22] LLC Gurobi Optimization. Gurobi optimizer reference manual, 2019. 2
[23] Roberto Henschel, Laura Leal-TaixeÃÅ, Daniel Cremers, and
Bodo Rosenhahn. Fusion of head and full-body detectors
for multi-object tracking. In IEEE Conference on Computer
Vision and Pattern Recognition Workshops, June 2018. 2
[24] Roberto Henschel, Timo von Marcard, and Bodo Rosenhahn.
Simultaneous identification and tracking of multiple people
using video and imus. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 0‚Äì0, 2019. 2
[25] Roberto Henschel, Yunzhe Zou, and Bodo Rosenhahn. Multiple people tracking using body and joint detections. In
IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 0‚Äì0, 2019. 2
[26] Kalun Ho, Amirhossein Kardoost, Franz-Josef Pfreundt, Janis Keuper, and Margret Keuper. A two-stage minimum cost
multicut approach to self-supervised multiple person tracking. In Proceedings of the Asian Conference on Computer
Vision (ACCV), November 2020. 2
[27] Martin Hofmann, Daniel Wolf, and Gerhard Rigoll. Hypergraphs for joint multi-view reconstruction and multi-object
tracking. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3650‚Äì3657, 2013. 2

[1] Ravindra K Ahuja, Thomas L Magnanti, and James B Orlin.
Network flows. Cambridge, Mass.: Alfred P. Sloan School of
Management, Massachusetts, 1988. 5, 6
[2] Alexandre Alahi, Yannick Boursier, Laurent Jacques, and
Pierre Vandergheynst. Sport players detection and tracking
with a mixed network of planar and omnidirectional cameras. In 2009 Third ACM/IEEE International Conference
on Distributed Smart Cameras (ICDSC), pages 1‚Äì8. IEEE,
2009. 1
[3] Alexandre Alahi, Judson Wilson, Li Fei-Fei, and Silvio
Savarese. Unsupervised camera localization in crowded
spaces. In 2017 IEEE International Conference on Robotics
and Automation (ICRA), pages 2666‚Äì2673. IEEE, 2017. 1
[4] Chetan Arora and Amir Globerson. Higher order matching
for consistent multiple target tracking. In Proceedings of the
IEEE International Conference on Computer Vision, pages
177‚Äì184, 12 2013. 2
[5] Maryam Babaee, Ali Athar, and Gerhard Rigoll. Multiple people tracking using hierarchical deep tracklet reidentification. arXiv preprint arXiv:1811.04091, 2018. 2
[6] Jerome Berclaz, Francois Fleuret, Engin Turetken, and Pascal Fua. Multiple object tracking using k-shortest paths optimization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(9):1806‚Äì1819, 2011. 2
[7] Philipp Bergmann, Tim Meinhardt, and Laura Leal-TaixeÃÅ.
Tracking without bells and whistles. In IEEE International
Conference on Computer Vision, pages 941‚Äì951, 2019. 2, 7,
8
[8] Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: The clear mot metrics.
EURASIP Journal on Image and Video Processing, 2008, 01
2008. 8
[9] Erik Bochinski, Volker Eiselein, and Thomas Sikora. Highspeed tracking-by-detection without using image information. In 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), pages
1‚Äì6. IEEE, 2017. 2
[10] Guillem BrasoÃÅ and Laura Leal-TaixeÃÅ. Learning a neural solver for multiple object tracking. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 6247‚Äì6257, 2020. 2, 8
[11] William Brendel, Mohamed Amer, and Sinisa Todorovic.
Multiobject tracking as maximum weight independent set.
In IEEE Conference on Computer Vision and Pattern Recognition, pages 1273‚Äì1280. IEEE, 2011. 2
[12] Visesh Chari, Simon Lacoste-Julien, Ivan Laptev, and Josef
Sivic. On pairwise costs for network flow multi-object tracking. In IEEE Conference on Computer Vision and Pattern
Recognition, pages 5537‚Äì5545, 2015. 2
[13] Chee-Yee Chong, Shozo Mori, and Donald B Reid. Forty
years of multiple hypothesis tracking-a review of key developments. In 2018 21st International Conference on Information Fusion (FUSION), pages 452‚Äì459. IEEE, 2018. 2
[14] Sunil Chopra and Mendu R Rao. The partition problem.
Mathematical Programming, 59(1):87‚Äì115, 1993. 2, 5

9

[28] Andrea Hornakova, Roberto Henschel, Bodo Rosenhahn,
and Paul Swoboda. Lifted disjoint paths with application
in multiple object tracking. In The 37th International Conference on Machine Learning (ICML), July 2020. 1, 2, 3, 6,
7, 8, 27, 28, 29
[29] Andrea HornÃåaÃÅkovaÃÅ, Jan-Hendrik Lange, and Bjoern Andres.
Analysis and optimization of graph decompositions by lifted
multicuts. In International Conference on Machine Learning, 2017. 2
[30] Weiming Hu, Xinchu Shi, Zongwei Zhou, Junliang Xing,
Haibin Ling, and Stephen Maybank. Dual L1-normalized
context aware tensor power iteration and its applications to
multi-object tracking and multi-graph matching. International Journal of Computer Vision, Oct 2019. 2
[31] Chang Huang, Bo Wu, and Ramakant Nevatia. Robust object
tracking by hierarchical association of detection responses.
In European Conference on Computer Vision, pages 788‚Äì
801. Springer, 2008. 2
[32] Hao Jiang, Sidney Fels, and James J Little. A linear programming approach for multiple object tracking. In 2007
IEEE Conference on Computer Vision and Pattern Recognition, pages 1‚Äì8. IEEE, 2007. 2
[33] Margret Keuper, Siyu Tang, Bjoern Andres, Thomas Brox,
and Bernt Schiele. Motion segmentation & multiple object
tracking by correlation co-clustering. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 42(1):140‚Äì153,
2018. 2
[34] Chanho Kim, Fuxin Li, Arridhana Ciptadi, and James M
Rehg. Multiple hypothesis tracking revisited. In Proceedings of the IEEE international conference on computer vision, pages 4696‚Äì4704, 2015. 2
[35] PeÃÅter KovaÃÅcs. Minimum-cost flow algorithms: an experimental evaluation. Optimization Methods and Software,
30(1):94‚Äì127, 2015. 1, 2
[36] Ratnesh Kumar, Guillaume Charpiat, and Monique Thonnat. Multiple object tracking by efficient graph partitioning.
In Asian Conference on Computer Vision, pages 445‚Äì460.
Springer, 2014. 2
[37] Laura Leal-TaixeÃÅ, Anton Milan, Ian Reid, Stephan Roth, and
Konrad Schindler. MOTChallenge 2015: Towards a benchmark for multi-target tracking. arXiv:1504.01942 [cs], Apr.
2015. arXiv: 1504.01942. 2, 7
[38] Laura Leal-TaixeÃÅ, Gerard Pons-Moll, and Bodo Rosenhahn.
Branch-and-price global optimization for multi-view multitarget tracking. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 1987‚Äì1994. IEEE, 2012. 2
[39] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu,
Sergio Casas, and Raquel Urtasun. Pnpnet: End-to-end perception and prediction with tracking in the loop. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 11553‚Äì11562, 2020. 1
[40] Tsung-Yi Lin, Priyal Goyal, Ross Girshick, Kaiming He, and
Piotr Dollar. Focal loss for dense object detection. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
PP:1‚Äì1, 07 2018. 7
[41] Qiankun Liu, Qi Chu, Bin Liu, and Nenghai Yu. Gsm:
Graph similarity model for multi-object tracking. In Chris-

[42]

[43]

[44]

[45]

[46]

[47]

[48]

[49]

[50]

[51]

[52]

[53]

10

tian Bessiere, editor, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI20, pages 530‚Äì536. International Joint Conferences on Artificial Intelligence Organization, 7 2020. Main track. 8
Wei-Lwun Lu, Jo-Anne Ting, James J Little, and Kevin P
Murphy. Learning to track and identify players from broadcast sports videos. IEEE transactions on pattern analysis
and machine intelligence, 35(7):1704‚Äì1716, 2013. 1
Andrew L. Maas, Awny Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural network acoustic models.
In Proceedings of the International Conference on Machine
Learning, Atlanta, Georgia, 2013. 6, 23, 24
Florian Meyer, Thomas Kropfreiter, Jason L Williams,
Roslyn Lau, Franz Hlawatsch, Paolo Braca, and Moe Z Win.
Message passing algorithms for scalable multitarget tracking. Proceedings of the IEEE, 106(2):221‚Äì259, 2018. 2
Anton Milan, Laura Leal-TaixeÃÅ, Ian Reid, Stephan Roth,
and Konrad Schindler. MOT16: A benchmark for multiobject tracking. arXiv:1603.00831 [cs], Mar. 2016. arXiv:
1603.00831. 2, 7, 28, 30
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 779‚Äì788, 2016. 1
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. arXiv preprint arXiv:1506.01497, 2015.
1
Seyed Hamid Rezatofighi, Anton Milan, Zhen Zhang, Qinfeng Shi, Anthony Dick, and Ian Reid. Joint probabilistic
data association revisited. In Proceedings of the IEEE international conference on computer vision, pages 3047‚Äì3055,
2015. 2
Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara,
and Carlo Tomasi. Performance measures and a data set for
multi-target, multi-camera tracking. In Gang Hua and HerveÃÅ
JeÃÅgou, editors, Computer Vision ‚Äì ECCV 2016 Workshops,
pages 17‚Äì35, Cham, 2016. Springer International Publishing. 8
Ergys Ristani, Francesco Solera, Roger S. Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and a data
set for multi-target, multi-camera tracking. In European
Conference on Computer Vision Workshop on Benchmarking
Multi-Target Tracking, 2016. 6
Ergys Ristani and Carlo Tomasi. Tracking multiple people
online and in real time. In Asian Conference on Computer
Vision, pages 444‚Äì459. Springer, 2014. 2
Amir Sadeghian, Alexandre Alahi, and Silvio Savarese.
Tracking the untrackable: Learning to track multiple cues
with long-term dependencies. In IEEE International Conference on Computer Vision, pages 300‚Äì311, 2017. 2
Julian Smith, Florian Particke, Markus Hiller, and JoÃàrn Thielecke. Systematic analysis of the pmbm, phd, jpda and gnn
multi-target tracking filters. In 2019 22th International Conference on Information Fusion (FUSION), pages 1‚Äì8, 2019.
2

learning for person re-identification. In IEEE Conference on
Computer Vision and Pattern Recognition, 2019. 6
[68] Xingyi Zhou, Vladlen Koltun, and Philipp KraÃàhenbuÃàhl.
Tracking objects as points. In European Conference on Computer Vision, pages 474‚Äì490. Springer, 2020. 2, 8
[69] Ji Zhu, Hua Yang, Nian Liu, Minyoung Kim, Wenjun Zhang,
and Ming-Hsuan Yang. Online multi-object tracking with
dual matching attention networks. In European Conference
on Computer Vision, pages 366‚Äì382, 2018. 2

[54] Paul Swoboda, Jan Kuske, and Bogdan Savchynskyy. A dual
ascent framework for lagrangean decomposition of combinatorial problems. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July
2017. 2, 3, 4
[55] Siyu Tang, Bjoern Andres, Miykhaylo Andriluka, and Bernt
Schiele. Subgraph decomposition for multi-target tracking.
In IEEE Conference on Computer Vision and Pattern Recognition, pages 5033‚Äì5041, 2015. 2
[56] Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, and Bernt
Schiele. Multi-person tracking by multicut and deep matching. In European Conference on Computer Vision, pages
100‚Äì111. Springer, 2016. 1, 2
[57] Siyu Tang, Mykhaylo Andriluka, Bjoern Andres, and Bernt
Schiele. Multiple people tracking by lifted multicut and person re-identification. In IEEE Conference on Computer Vision and Pattern Recognition, 2017. 1, 2
[58] Timo von Marcard, Roberto Henschel, Michael J Black,
Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and a moving camera. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 601‚Äì617, 2018. 2
[59] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person reidentification. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 79‚Äì88, 2018. 6
[60] Nicolai Wojke and Alex Bewley. Deep cosine metric learning for person re-identification. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages
748‚Äì756. IEEE, 2018. 2
[61] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple
online and realtime tracking with a deep association metric.
In 2017 IEEE International Conference on Image Processing
(ICIP), pages 3645‚Äì3649. IEEE, 2017. 2
[62] Jiarui Xu, Yue Cao, Zheng Zhang, and Han Hu. Spatialtemporal relation networks for multi-object tracking. In
IEEE International Conference on Computer Vision, pages
3988‚Äì3998, 2019. 2
[63] Fan Yang, Wongun Choi, and Yuanqing Lin. Exploit all the
layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2129‚Äì2137, 2016. 1
[64] Amir Roshan Zamir, Afshin Dehghan, and Mubarak Shah.
GMCP-tracker: Global multi-object tracking using generalized minimum clique graphs. In European Conference on
Computer Vision, pages 343‚Äì356. Springer, 2012. 2
[65] Li Zhang, Yuan Li, and Ramakant Nevatia. Global data association for multi-object tracking using network flows. In
IEEE Conference on Computer Vision and Pattern Recognition, pages 1‚Äì8. IEEE, 2008. 1, 2
[66] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person re-identification:
A benchmark. In IEEE International Conference on Computer Vision, pages 1116‚Äì1124, 2015. 6
[67] Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng,
Yi Yang, and Jan Kautz. Joint discriminative and generative

11

Input
Detections

Output
Trajectories

ApLift

Appearance Similarity

√ó

Global Context
Normalization

Spatial Similarity
MLP

ùë°1

Pairwise Cost
Obtain Costs

ùë°2

ùë°3

ùë°4

Create Sparse Graphs

ùë°1

ùë°2

ùë°3

ùë°4

Solve LDP

Figure 1. Overview of the ApLift framework. Input detections are used to obtain pairwise costs by an MLP with spatial and appearance
features. Based on the costs, two sparse graphs are constructed and passed to our proposed approximate LDP solver. Dashed arrows
represent lifted edges and solid arrows base edges. In figure Solve LDP equally colored nodes and edges belong to the same trajectory.

8. Appendix

subproblems are obtained by modifications of the respective algorithms for their optimization. See Section 4.4 in
the main text for the cut subproblem optimization and Section 8.3 for path subproblem optimization.
Tightening by separation. We create the new path and
cut subproblems in order to tighten the LP relaxation of the
problem (3). Section 8.6 discusses the guaranteed lower
bound improvement achieved by separating the new subproblems using algorithms in Sections 8.4 and 8.5.
Tracking. The proposed tracking framework contains additional processing steps, which are briefly mentioned in the
main paper. A detailed description and additional evaluation data is provided in this appendix. To construct the
graph we calculate costs based on two features as described
in Section 5.1 and add multiple scalings which details can
be found in Section 8.9. We also determine very confident edges and set their cost based on heuristics explained
in Section 8.12. Furthermore, additional implementation
and training details for the classifier are presented in Sections 8.10 and 8.11. The efficient inference based on interval solutions is provided in Section 8.13.1. Finally we
show details for the post-processing based on heuristics in
Section 8.13.2.

This Appendix contains details about our approximate
LDP solver and the whole MOT framework used in ApLift.
We depicts this framework in Figure 1.
Appendix outline. We start with providing additional notation and abrreviations list in Section 8.1. Sections 8.28.8 present the message passing solver implementation and
the algorithms used for it. Sections 8.9-8.13.2 present details about processing of the tracking data. Finally, Section 8.14 discusses theoretical runtime of the solver and
Section 8.15 presents examples of qualitative results. The
Appendix is rather extensive, especially its algorithmic part.
Therefore, we provide its section outline within the context
of the whole method bellow.
LDP solver outline. Figure 2 contains a scheme of all algorithms used in our LDP solver. The algorithms are stated
either in the main paper or in this Appendix. The solver
performs an explicitly given number of message passing iterations. Section 8.7 describes the full solver run and an
overview of all methods used within one message passing iteration. Once in five iterations, new primal solution is computed (Sections 4.6 and 8.8). Once in twenty iterations, new
subproblems are separated and added to the problem. These
are path and cut subproblems (see Sections 4.3 and 4.4).
Methods for their separations are described in Sections 8.4
and 8.5.
Message passing.
Messages are sent between the subproblems. Each subproblem creates messages to be sent
by computing min-marginals of its variables. Section 8.2
presents algorithms used for obtaining min-marginals of inflow and outflow subproblems. The algorithms allow us
to efficiently obtain min-marginals of all lifted or all base
edges of a subproblem at once. Messages from cut and path

8.1. Additional notation and abbreviations
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
12

x ‚àà AB denotes a mapping x : B ‚Üí A.
[n] denotes the set of numbers {1, 2, . . . , n}.
LP: Linear programming.
MP: Message passing.
DP: disjoint paths problem.
LDP: Lifted disjoint paths.
DFS: Depth first search.
MPLP: Max Product Linear Programming.

Figure 2. The scheme of our message passing algorithm and all its subroutines described in this work. An arrow from Algorithm X to
Algorithm Y means that Algorithm X calls Algorithm Y . Algorithms in brackets denote that their modifications are used as the respective
procedures. Abbreviation MP means Messate-Passing. Some algorithms for inflow subproblems are omitted for clarity because they are
analogical to outflow subproblem algorithms.

13

0
The cost of lifted edge Œ∏ÃÉvu
must be subtracted because it is
involved in both values lifted cost[u] and back cost[u].
Algorithm 11 performs two tasks simultaneously. First,
it is a DFS procedure for computing back cost. Contrary
to Algorithm 2 that performs DFS for obtaining lifted cost,
Algorithm 11 proceeds in the opposite edge direction. It
again uses the fact that a subpath of a minimum-cost
path must be minimal. Second, it directly computes min
marginal for already processed vertex u on Line 10 and involves this change in setting back cost[u] on Line 11.
Speeding up DFS: All the algorithms for obtaining optimal solution or min-marginals of inflow and outflow subproblems call DFS procedures. It can be considered that the
order of processing the relevant nodes reachable form the
central node is always the same. Therefore, we call DFS
for each inflow and outflow subproblem only once during
their initialization and store the obtained list of processed
nodes. The full DFS in Algorithm 2 is replaced by traversing the precomputed node list in the forward direction. Algorithm 11 is replaced by traversing this node list in the
backward direction.

8.2. Min-Marginals for Inflow and Outflow Subproblems
We detail routines for computing min-marginals for all
base edges at once (Algorithm 5) and all lifted edges at once
(Algorithm 6). All the stated algorithms assume outflow
subproblems. Modification to inflow subproblems is done
via proceeding in the oposite edge direction.
Iteratively computing min-marginals and performing operation (6) would be inefficient, since it would involve call+
+
ing Algorithm 1 O(|Œ¥E
(v)| + |Œ¥E
0 (v))| times. To speed up
iterative min-marginal updates, we can reuse computations
as done in Algorithm 5 for base edges and in Algorithm 6
for lifted edges.
Algorithm 5 for computing base edge min-marginals
uses the fact that lifted edge costs do not change and therefore Algorithm 1 needs to be called only once. For lifted
edges, Algorithm 6 interleaves min-marginal computation
and reparametrization updates (6) such that computations
0
in
can be reused. We introduce auxiliary variables Œ≥vw
line 3 that keep track of future reparametrization updates.
For the min-marginals, we will need slight extensions of
Algorithm 1 and a method to additionally compute a labeling that attains the optimum. These methods are given in
Algorithm 9 and 7.
In Algorithm 6, path P ‚àó representing the optimal solution of the outflow problem is found by calling Algorithm 1
followed Algorithm 7. Then, Algorithm 8 computes minmarginals for the lifted edges that are active in the optimal
solution. In the end of Algorithm 6, min-marginals are computed for those lifted edges that are not active in the optimal
solution.
For computing min-marginals of edges that are active in
the optimal solution, we need as a subroutine Algorithm 9,
an extended version of Algorithm 1. Algorithm 9 restricts
the vertices taken into consideration during the optimization. In particular, a special vertex r is given that is to be
excluded from the optimization. Values lifted cost[u] are
reused for those vertices u where ur ‚àà
/ RG because these
values are not affected by excluding vertex r.
Min-marginals for vertices inactive in the optimal solution are computed by Algorithms 10 and 11. The algorithms rely on structure back cost which is an analogy of
lifted cost. Structure back costs[u] contains the minimum
cost of all vu-paths w.r.t. to the costs of all lifted edges
connecting v with the vertices of the path plus the cost of
the first base edge of the path. Note that lifted costs[u] is
defined analogically but contains the minimum cost of all
ut-paths. Therefore, the minimal solution where a lifted
edge vu ‚àà E 0 is active can be obtained as follows:
min

Algorithm 5 All-Base-MM-Out(v, Œ∏ÃÉ)
Input start vertex v, costs Œ∏ÃÉ
+
Output base edge min-marginals Œ≥vu ‚àÄvu ‚àà Œ¥E
(v)
out
)
1: (opt, lifted cost, Œ±) =Opt-Out-cost(v, Œ∏
2: e‚àó = argmin{Œ±vw }, e‚àó‚àó = argmin {Œ±vw }
3:

+
\{e‚àó }
vw‚ààŒ≥E

‚àÄvu ‚àà Œ¥ (v) : Œ≥vu = Œ±vu ‚àí min(Œ±e‚àó‚àó , 0)

Algorithm 6 All-Lifted-MM-Out(v, Œ∏ÃÉ)
Input starting vertex v, Œ∏ÃÉ
+
0
‚àÄvu ‚àà Œ¥E
Output lifted edge min-marginals Œ≥vu
0 (v)
1: (opt, lifted cost, Œ±, next) =Opt-Out-cost(v, Œ∏ out )
2: PV‚àó =Get-Opt-Path-Out(Œ∏ out , Œ±, next)
+
0
=0
3: ‚àÄvw ‚àà Œ¥E 0 (v) : Œ≥vw
4: (opt, Œ≥ 0 ) =MM-Opt-Out(v, PV‚àó , opt, Œ≥ 0 , Œ∏ÃÉ)
5: Œ≥ 0 =MM-Not-Opt-Out(v, opt, Œ≥ 0 , Œ∏ÃÉ)

h(zv , y, y 0 ), Œ∏out i =

0 =1
(z,y,y 0 )‚ààXvout :yvu

0
= lifted cost[u] + back cost[u] ‚àí Œ∏ÃÉvu

+
vw‚ààŒ≥E
+

(11)
14

Algorithm 7 Get-Opt-Path-Out
Input costs Œ∏ÃÉ, vector Œ± such that ‚àÄvw ‚àà
optimal value if vw is active, next
Output min cost path PV‚àó
1: w ‚àó = argminw:vw‚ààŒ¥ + (v) Œ±vw
E
2: if Œ±vw‚àó < 0 then
3:
while w‚àó 6= t do
4:
PV‚àó ‚Üê w‚àó
5:
w‚àó = next[w‚àó ]
6:
end while
7: else
8:
PV‚àó = ‚àÖ
9: end if

+
Œ¥E
(v)

Algorithm 10 MM-Not-Opt-Out
Input v, current optimum opt, reparametrization update Œ≥ 0 ,
Œ∏ÃÉ
Output changed reparametrization update Œ≥ 0
1: (opt, lifted cost) =Opt-Out-cost(v, Œ∏ÃÉ ‚àí (0, Œ≥ 0 ))
2: for all u : vu ‚àà RG do
3:
if u ‚àà PV‚àó then
4:
visited[u] = true
5:
back cost[u] = opt ‚àí lifted cost[u]
0
0
‚àí Œ≥vw
6:
if vu ‚àà E 0 then back cost[u] += Œ∏ÃÉvu
7:
else
8:
visited[u] = f alse
+
9:
if vu ‚àà Œ¥E
(v) then
10:
back cost[u] = Œ∏ÃÉvu
11:
else
12:
back cost[u] = ‚àû
13:
end if
14:
end if
15: end for
+
16: for all vu ‚àà Œ¥E 0 (v) do
17:
if visited[u] = f alse then
18:
Backward-DFS(v, u, Œ∏ÃÉ, Œ≥ 0 , opt, back cost)
19:
end if
20: end for

: Œ±vw is the

Algorithm 8 MM-Opt-Out
Input starting vertex v, optimal path PV‚àó = (v1 , . . . , vk ),
value of optimal path opt, Œ≥ 0 , costs Œ∏ÃÉ
Output updated cost of optimal path opt, new
reparametrization updates Œ≥ 0
+
1: for all vi = v1 , . . . , vk : vvi ‚àà Œ¥E 0 (v) do
2:
Œ± = Skip-One(v, vi , Œ∏ÃÉ ‚àí (0, Œ≥ 0 ), lifted cost, next)
0
= opt ‚àí Œ±
3:
Œ≥vv
i
4:
opt = Œ±
5: end for

Algorithm 11 Backward-DFS
Algorithm 9 Skip-One

Input v, u, Œ∏ÃÉ, Œ≥ 0 , opt, back cost
Output Œ≥ 0 , back cost
1: Œ± = back cost[u]
‚àí
2: for wu ‚àà Œ¥E (u) : vw ‚àà RG do
3:
if visited[w] = f alse then
4:
Backward-DFS(v, w, Œ∏ÃÉ, Œ≥ 0 , back cost)
5:
end if
6:
Œ± = min{back cost[w], Œ±}
7: end for
8: if vu ‚àà E 0 then
9:
optu = Œ± + lifted cost[u]
0
10:
Œ≥vu
= optu ‚àí opt
0
11:
back cost[u] = Œ± + Œ∏ÃÉvu ‚àí Œ≥vu
12: else
13:
back cost[u] = Œ±
14: end if
15: visited[u] = true

Input v, ignored vertex r, Œ∏ÃÉ, lifted cost, next
Output optimal value opt
1: for u ‚àà V : vu ‚àà RG ‚àß ur ‚àà RG do
2:
lifted cost[u] = ‚àû, next[u] = ‚àÖ
3: end for
4: lifted cost[r] = 0, next[r] = t
5: Lifted-Cost-DFS-Out(v, v, Œ∏ÃÉ, lifted cost, next)
+
6: ‚àÄw : vw ‚àà Œ¥E (v) : Œ±vw = Œ∏ÃÉv + Œ∏ÃÉvw + lifted cost[w]
7: opt = min(minw:vw‚ààŒ¥ + (v) Œ±vw , 0)
E

8.3. Optimization of path subproblems.
We denote by Œ∏P the edge costs in subproblem of
vw-path P . The optimization over the feasible set X P w.r.t.
costs Œ∏P is detailed in Algorithm 12. It checks whether
there exists exactly one positive edge and whether it is either a lifted or a strong base edge (Line 2). If so, the optimal
solution is either (i) all edges except the two largest ones
(Line 6) or (ii) all edges (Line 8), whichever gives smaller
objective value. If the above condition does not hold, the
optimal solution can be chosen to contain all negative edges
(Line 11).
A variation of Algorithm 12 with a specified edge fixed
to either 0 or 1 is used for computing min-marginals

8.4. Separation for Path Subproblems
The path subproblem separation procedure is described
in Algorithm 14. The algorithm finds paths together with a
lifted edge connecting the start and the end point of the path
such that exactly one lifted edge has positive cost, while all
the remaining base and lifted edges have negative cost.
15

Algorithm 12 Path-Subproblem-Optimization
Input Edge costs Œ∏P
Output optimal value opt of subproblem.
0P
P
1: E + = {kl ‚àà PE 0 ‚à™ vw|Œ∏kl
> 0} ‚à™ {kl ‚àà PE |Œ∏kl
> 0}
+
2: if E = {kl} ‚àß kl ‚àà PE 0 ‚à™ vw ‚à™ E0 then
P
0P
3:
Œ± = min{ min |Œ∏ij
|,
min
|Œ∏ij
|}
ij‚ààPE \E +
ij‚ààPE 0 ‚à™vw\E +
(
0P
Œ∏kl
, kl ‚àà PE 0 ‚à™ vw
4:
Œ≤=
P
Œ∏kl , kl ‚àà PE
5:
if Œ± < Œ≤ then P
P
P
0P
6:
opt =
Œ∏ij
+
Œ∏ij
+Œ±
ij‚ààPE \E +

7:
8:

else
P P
opt =
Œ∏ij +
ij‚ààPE

9:
10:
11:

end if
else
opt =

P
ij‚ààPE \E +

12:
13:

P
Œ∏ij
+

Algorithm 13 Separation-Costs
Input Current cost in inflow and outflow factors Œ∏in , Œ∏out
Output Cost reparametrization ‚àÄuv ‚àà E : Œ∏ÃÉuv , ‚àÄuv ‚àà E 0 :
0
Œ∏ÃÉuv
0
1: ‚àÄuv ‚àà E : Œ∏ÃÉuv = 0, ‚àÄuv ‚àà E 0 : Œ∏ÃÉuv
=0
2: for all u ‚àà V \ {s, t} do
+
out
3:
‚àÄuv ‚àà Œ¥E
(u) : Œ≥uv
=0
‚àí
in
4:
‚àÄuv ‚àà Œ¥E (u) : Œ≥vu
=0
5:
Œ≥u0out = 0.5¬∑All-Lifted-MM-Out(u, Œ∏uout )
6:
Œ≥u0in = 0.5¬∑All-Lifted-MM-In(u, Œ∏uin )
7:
Œ≥uout =All-Base-MM-Out(u, Œ∏uout ‚àí (Œ≥uout , Œ≥u0out ))
8:
Œ≥uin =All-Base-MM-In(u, Œ∏uin ‚àí (Œ≥uin , Œ≥u0in ))
+
out
9:
‚àÄuv ‚àà Œ¥E
(u) : Œ∏ÃÉuv += Œ≥uv
‚àí
in
10:
‚àÄuv ‚àà Œ¥E (u) : Œ∏ÃÉvu += Œ≥vu
+
0
0out
11:
‚àÄuv ‚àà Œ¥E
0 (u) : Œ∏ÃÉuv += Œ≥uv
‚àí
0
0in
12:
‚àÄuv ‚àà Œ¥E 0 (u) : Œ∏ÃÉvu += Œ≥vu
13: end for

ij‚ààPE 0 ‚à™vw\E +

P
ij‚ààPE 0 ‚à™vw

0P
Œ∏ij

P

0P
Œ∏ij

ij‚ààPE 0 ‚à™vw\E +

end if
return opt

Algorithm 14 Separate-Path-Subproblem
Input Cost threshold Œµ
1: Œ∏ÃÉ =Separation-Costs(Œ∏ in , Œ∏ out )
2: G1 = (V, E 1 = ‚àÖ)
0
3: E ‚àí = {vw ‚àà E|Œ∏ÃÉvw < ‚àíŒµ} ‚à™ {vw ‚àà E 0 |Œ∏ÃÉvw
< ‚àíŒµ}
0+
0 0
4: E
= {vw ‚àà E |Œ∏ÃÉvw > Œµ}
5: ‚àÄv ‚àà V : desc[v] = {v}, pred[v] = {v}
6: Priority-Queue Q = ‚àÖ
7: for all ij ‚àà E ‚àí ascending in Œ∏ÃÉ do
8:
if ij ‚àà E then cij = Œ∏ÃÉij
0
9:
else cij = Œ∏ij
10:
Inner-Paths(i, j, cij , pred, desc, E + , E 1 , Q)
11:
Outer-Paths(i, j, cij , pred, desc, E + , E 1 , Q)
12:
Connect(i, j, pred, desc, E 1 )
13: end for

First, lifted and base edge costs are obtained in Algorithm 13 by computing min-marginals of inflow and outflow factors. Second, a graph with an empty edge set E 1
is created. Then, edges with negative costs are added to
E 1 in ascending order. After adding an edge, we check
whether separating path subproblems with edge costs leading to lower bound improvement is possible. Such a factor
must contain the newly added edge, one positive lifted edge
and edges that already are in the edge set E 1 .
Algorithm 15 separates those paths subproblems where
the only positive edge is the one connecting the path‚Äôs endpoints. Algorithm 16 separates those path subproblems
where the only positive edge is one of the edges within
the path. Algorithm 17 updates connectivity structures by
adding edge ij to the edge set E 1 .
Each path subproblems has a guaranteed lower bound
improvement, see Proposition 2. We add each found path
subproblem to priority queue Q, where we sort w.r.t. the
guaranteed lower bound improvement. After searching for
path subproblems, we add the k best path subproblems from
queue Q to the optimization problem.

Algorithm 15 Inner-Paths
Input i, j, cij , pred, desc, E + , E 1 , Q
1: for all p ‚àà pred[i] do
2:
for all d ‚àà desc[j] do
3:
if pd ‚àà E + then
4:
P1 =Find-Path(p, i, E 1 )
5:
P2 =Find-Path(j, d, E 1 )
6:
P = (P1 , ij, P2 )
0
7:
priority = min{|cij |, Œ∏ÃÉpd
}
8:
Q ‚Üê (Path-Problem(P ), priority)
9:
end if
10:
end for
11: end for

8.5. Separation for Cut Subproblems
Algorithm 18 separates cut subproblems. The algorithm
finds cuts consisting of base edges with positive costs and
a lifted edge having endpoints on both sides of the cut and
negative cost. Similarly as for the path subproblem separation, lifted and base edge costs are obtained by computing min-marginals of inflow and outflow factors in Algorithm 13. Each edge uv ‚àà E 0‚àí is a candidate lifted edge for
a uv-cut factor.

The edge set E 1 initially contains all base edges with
cost lower than Œµ. The remaining base edges are added to
16

Algorithm 16 Outer-Paths
Input i, j, cij , pred, desc, E + , E 1 , Q
1: for all p ‚àà pred[j] do
2:
for all d ‚àà desc[i] do
3:
if dp ‚àà E + then
4:
P1 =Find-Path(i, d, E 1 )
5:
P2 =Find-Path(p, j, E 1 )
6:
P = (P1 , ij, P2 )
0
7:
priority = min{|cij |, Œ∏ÃÉdp
}
8:
Q ‚Üê (Path-Problem(P ), priority)
9:
end if
10:
end for
11: end for

Algorithm 18 Separate-Cut-Subproblem
Input Cost threshold Œµ
1: Œ∏ÃÉ =Separation-Costs(Œ∏ in , Œ∏ out )
0
2: E 0‚àí = {vw ‚àà E 0 |Œ∏ÃÉvw
< ‚àíŒµ}
‚àí
0
3: E = {vw ‚àà E|Œ∏ÃÉvw < Œµ}, E + = E \ E ‚àí
4: E 1 = E ‚àí , G1 = (V, E 1 )
5: Priority-Queue Q = ‚àÖ
6: for all ij ‚àà E + ascending in Œ∏ÃÉ do
7:
for all u ‚àà pred[i] do
8:
for all v ‚àà desc[j] do
9:
if uv ‚àà E 0‚àí then
10:
C= cut between u, v using edges E \ E 1
0
11:
priority = min{Œ∏ÃÉij , |Œ∏ÃÉuv
|}
12:
Q ‚Üê (Cut-Problem(C, u, v), priority)
13:
end if
14:
end for
15:
end for
16:
Connect(i, j, pred, desc, E 1 )
17: end for

Algorithm 17 Connect
Input i, j, pred, desc, E 1
1: for all p ‚àà pred[i] do
2:
for all d ‚àà desc[j] do
3:
desc[p] ‚Üê d
4:
pred[d] ‚Üê p
5:
E 1 ‚Üê ij
6:
end for
7: end for

1. ‚àÄi ‚àà [ds ]

(
‚â§0
Œ≥i =
‚â•0

if ‚àÉx‚àó ‚àà argminx‚ààX s hŒ∏, xi : x‚àói = 1
if ‚àÉx‚àó ‚àà argminx‚ààX s hŒ∏, xi : x‚àói = 0
(12)
2. argminhŒ∏, xi ‚äÜ argminhŒ∏ ‚àí Œ≥, xi

E 1 in ascending order. Whenever a newly added edge ij
causes a connection between u and v where uv ‚àà E 0‚àí , a
uv-cut C is separated. We select the cut C to contain only
those edges that do not belong to E 1 . This ensures that ij is
the weakest cut edge. In the same time, C is the best uv-cut
with respect to the cost of the weakest cut edge.
The found cut factors are added to a priority queue where
the priority represents guaranteed lower bound improvement (see Proposition 3) after adding the factor to our problem.

x‚ààX s

x‚ààX s

and a coordinate-wise scaled reparametrization Œ≥(œâ) defined by coefficients œâ ‚àà [0, 1]s where ‚àÄi ‚àà [ds ] : Œ≥(œâ)i =
œâi Œ≥i , it holds:
1. The lower bound of s after
P reparametrization Œ≥(œâ) is
Ls (Œ∏ ‚àí Œ≥(œâ)) = Ls (Œ∏) ‚àí i‚àà[ds ]:Œ≥i <0 œâi Œ≥i .
2. argminhŒ∏, xi ‚äÜ argminhŒ∏ ‚àí Œ≥(œâ), xi

8.6. Tightening Lower Bound Improvement

x‚ààX s

In order to show that the separation procedures in Algorithms 14 and 18 lead to relaxations that improve the lower
bound we show the following: (i) Certain reparametrization used in the above algorithms are non-decreasing in the
lower bound. (ii) Separation procedures find new subproblems such that w.r.t. the above reparametrization, a guaranteed lower bound improvement can be achieved.
Points (i) and (ii) guarantee that the same lower bound
achievement w.r.t. the original reparametrization can be
found. The special reparametrization chosen helps empirically to find good subproblems.

x‚ààX s

Proof. We start with evaluating hŒ∏ ‚àí Œ≥(œâ), x‚àó i where x‚àó ‚àà
argminx‚ààX s hŒ∏, xi.

hŒ∏ ‚àí Œ≥(œâ), x‚àó i =

X

(Œ∏i ‚àí œâi Œ≥i )x‚àói =

i‚àà[ds ]

=

X
i‚àà[ds ]

Lemma 1. Let s ‚àà S be a subproblem, Œ∏ its cost and Ls (Œ∏)
its lower bound for cost Œ∏. Given a cost reparametrization
Œ≥ such that

X

Œ∏i x‚àói ‚àí

= Ls (Œ∏) ‚àí

i‚àà[ds ]:Œ≥i <0

X
i‚àà[ds ]:Œ≥i <0

17

œâi Œ≥i x‚àói =

œâi Œ≥i

(13)

‚àÄx ‚àà X , ‚àÄx‚àó ‚àà argminx‚ààX s hŒ∏, xi :
X
hŒ∏ ‚àí Œ≥(œâ), xi =
(Œ∏i ‚àí œâi Œ≥i )xi =

P
C
Œ∏uv
= œâuv Œ∏ÃÉuv , resp. Œ∏uv
= œâuv Œ∏ÃÉuv . Analogically, for the
lifted edges.
Cost update in in/outflow subproblems. If we use an
edge uv for creating one or more path and cut suproblems,
it is necessary to update its cost in the inflow subproblem
of vertex v and the outflow subproblem of vertex u accordingly. For instance, we update the cost of base edge uv
out
out
in the outflow subproblem of u as follows Œ∏uv
‚àí= Œ≥uv
.
Where we adopt the notation from Algorithm 13. Note that
in
out
Œ∏ÃÉuv = Œ≥uv
+ Œ≥uv
. Therefore, the total cost of edge variable
uv is preserved.

i‚àà[ds ]

X

=

X

(Œ∏i ‚àí Œ≥i )xi +

i‚àà[ds ]

(1 ‚àí œâi )Œ≥i xi ‚â•

i‚àà[ds ]

X

‚â• Ls (Œ∏ ‚àí Œ≥) +

(1 ‚àí œâi )Œ≥i xi =

i‚àà[ds ]:Œ≥i <0

X

=

X

(Œ∏i ‚àí Œ≥i )x‚àói +

i‚àà[ds ]

(1 ‚àí œâi )Œ≥i x‚àói =

i‚àà[ds ]
‚àó

= hŒ∏ ‚àí Œ≥(œâ), x i

Proposition 2 (Guaranteed lower bound improvement of path subproblem). If a path subproblem
corresponding to vw-path P separated by Algorithm 14 is added to the subproblem set S, the guaranteed improvement of the global lower bound is
P
0P
min{minuv‚ààPE |Œ∏uv
|, minuv‚ààPE0 ‚à™vw |Œ∏uv
|}, where Œ∏P
is the reparametrized cost used for the path factor
initialization.

(14)

Formula 14 proves Point 2 of Lemma 1. Formulas 13 and
14 together prove Point 1.
Lemma 2. Variables (Œ≥uout , Œ≥u0out ), resp. (Œ≥uin , Œ≥u0in ) in Algorithm 13 satisfy the requirements of Lemma 1 for each
outflow resp. inflow subproblem of vertex u.
Proof. Both Algorithms 5 and 6 output reparametrization
variables that satisfy the requirements of Lemma 1. We
have, for an outflow subproblem of node u:

Proof. Algorithm 14 separates only those subproblems that
0P
> Œµ and
contain exactly one lifted edge with cost Œ∏uv
the rest of the edges have cost lower than ‚àíŒµ. The
reparametrized costs of the path factor are fractions of cost
reparametrizations obtained by Algorithm 13. We have

argmin hŒ∏, (y, y 0 )i ‚äÜ argmin hŒ∏‚àí(0, Œ≥u0out ), (y, y 0 )i
(y,y 0 )‚ààXuout

(y,y 0 )‚ààXuout

‚äÜ argmin hŒ∏ ‚àí (Œ≥uout , Œ≥u0out ), (y, y 0 )i (15)

‚àÄuv ‚ààPE :

(y,y 0 )‚ààXuout

P
out
in
Œ∏uv
= œâuv ¬∑ (Œ≥uv
+ Œ≥uv
),

Therefore, also (Œ≥uout , Œ≥u0out ) together satisfy the requirements of Lemma 1. Analogically, for the inflow subproblems.

out
out
in
in
Œ∏uv
‚àí= œâuv ¬∑ Œ≥uv
, Œ∏uv
‚àí= œâuv ¬∑ Œ≥uv
,

‚àÄuv ‚ààPE 0 :

(18)

0P
0
0out
0in
Œ∏uv
= œâuv
¬∑ (Œ≥uv
+ Œ≥uv
),

Costs in the new path and cut subproblems. One edge is
typically shared among multiple newly added path and cut
subproblems. Therefore, the available cost reparametrizations Œ∏ÃÉ and Œ∏ÃÉ0 from Algorithm 13 must be redistributed to
the newly added subproblems. We denote the set of all
newly added path subproblems resp. cut subproblems in
tightening iteration i by P i resp. C i . For each base resp.
lifted edge uv, we sum up the number of newly added path
and cut subproblems that contain uv.

0out
0
0out
0in
0
0in
Œ∏uv
‚àí= œâuv
¬∑ Œ≥uv
, Œ∏uv
‚àí= œâuv
¬∑ Œ≥uv

We evaluate the change of the lower bounds of all relevant inflow and outflow factors after reparametrization
given by Formula 18. According to Lemma 1, we have
‚àÜLout + ‚àÜLin =
X
out
‚àí
œâuv ¬∑ Œ≥uv
‚àí

0
Nuv

i

‚àí

i

=|{P ‚àà P : uv ‚àà PE 0 }| + |{P ‚àà P : P is a uv-path}|
i

+ |{C ‚àà C : C is a uv-cut}| .

(16)

œâuv

0
œâuv

1
= 0 .
Nuv

in
œâuv ¬∑ Œ≥uv
‚àí

X

0
0in
œâuv
¬∑ Œ≥uv
‚â•

0in <0
uv‚ààPE 0 ‚à™vw:Œ≥uv

in <0
uv‚ààPE :Œ≥uv

‚àí

X

out
in
œâuv ¬∑ (Œ≥uv
+ Œ≥uv
)‚àí

out +Œ≥ in <0
uv‚ààPE :Œ≥uv
uv

0
Then, we define coefficient œâuv resp. œâuv
for each base
0
edge uv ‚àà E resp. lifted edge uv ‚àà E that belongs to a
newly added subproblem as

1
,
=
Nuv

X

0
0out
œâuv
¬∑ Œ≥uv

0out <0
uv‚ààPE 0 ‚à™vw:Œ≥uv

out <0
uv‚ààPE :Œ≥uv

Nuv =|{P ‚àà P i : uv ‚àà PE }| + |{C ‚àà C i : uv ‚àà CE }|,

(19)
X

X

‚àí

0
0out
0in
œâuv
¬∑ (Œ≥uv
+ Œ≥uv
)=

0in +Œ≥ 0out <0
uv‚ààPE 0 ‚à™vw:Œ≥uv
uv

‚àí

(17)

X
P <0
uv‚ààPE :Œ∏uv

P
œâuv ¬∑ Œ∏uv
‚àí

X

0
0P
œâuv
¬∑ Œ∏uv

0P <0
uv‚ààPE 0 ‚à™vw:Œ∏uv

Let kl ‚àà PE 0 be the only lifted edge with positive cost in the path subproblem.
We set Œ± =

Finally, for each newly added path subproblem P resp. cut
subproblem C, we set the cost of base edge uv ‚àà E to
18

P
min{ min |Œ∏ij
|,
ij‚ààPE

min

ij‚ààPE 0 ‚à™vw\kl
P

0P
|Œ∏ij
|} as in Algorithm 12. If

Algorithm 3 shows how we obtain the lower bound of the
C
C
C
cut subproblem. We set Œ∏ij
= argminuv‚ààC Œ∏uv
. If Œ∏ij
<
0C
|Œ∏vw |, we get the overall lower bound improvement

we denote by ‚àÜL the lower bound of the path subproblem, the global lower bound change after adding the path
subproblem is:
‚àÜL = ‚àÜL
If Œ±

out

in

+ ‚àÜL

+ ‚àÜL

P

0C
0C
C
C
‚àÜLout + ‚àÜLin + ‚àÜLC ‚â• ‚àíŒ∏vw
+ Œ∏vw
+ Œ∏ij
= Œ∏ij
.

(20)

C
0C
If Œ∏ij
‚â• |Œ∏vw
|, the lower bound of the cut subproblem is 0
and the overall lower bound improvement is

0P
< Œ∏kl
out

+ ‚àÜLin + ‚àÜLP ‚â•
X
P
œâuv ¬∑ Œ∏uv
‚àí

‚àÜL

‚àí

X

0C
‚àÜLout + ‚àÜLin + ‚àÜLC ‚â• ‚àíŒ∏vw
.

0
0P
œâuv
¬∑ Œ∏uv
+

P <0
uv‚ààPE :Œ∏uv

0P <0
uv‚ààPE 0 :Œ∏uv

X

X

(21)
+

P
œâuv ¬∑ Œ∏uv
+

P <0
uv‚ààPE :Œ∏uv

8.7. Message Passing

0
0P
œâuv
¬∑ Œ∏uv
+

One solver run consists of subproblems initialization and
a number of message passing iterations. Algorithm 19 details the whole run. Algorithms 23-22 present methods that
are called within one iteration.
The number of iterations is predetermined by an input
parameter. We use typically tens or maximally one hundred
iterations in our experiments.
Algorithm 19 sends in each iteration, messages between
all subproblems in the subproblem set S. Each subproblem
creates messages to be sent by computing min-marginals of
its variables (see Formula (5)). These min-marginals are
re-scaled and redistributed between other subproblems that
contain the respective variables. These operations are called
reparametrization. See Section 4 for details.
Algorithm 23 computes lower bound of the LDP objective by summing up lower bounds of all subproblems.
The cost reparametrization realized via our message passing
procedures ensures that the lower bound is non-decreasing
during the computaiton.
Algorithm 20 shows sending messages from the inflow
subproblem of node u. Algorithm 21 shows sending messages from a path subproblem. Algorithm 22 presents sending messages from a cut subproblem.

0P <0
uv‚ààPE 0 :Œ∏uv

+Œ±=Œ±
0P
If Œ± ‚â• Œ∏kl

‚àÜLout + ‚àÜLin + ‚àÜLP ‚â•
X
P
‚àí
œâuv ¬∑ Œ∏uv
‚àí

X

0
0P
œâuv
¬∑ Œ∏uv
+

0P <0
uv‚ààPE 0 ‚à™vw:Œ∏uv

P <0
uv‚ààPE :Œ∏uv

(22)
X

+

P
œâuv ¬∑ Œ∏uv
+

P <0
uv‚ààPE :Œ∏uv

=

X

0
0P
œâuv
¬∑ Œ∏uv
=

0P
uv‚ààPE 0 :Œ∏uv

0P
Œ∏kl

Proposition 3 (Guaranteed lower bound improvement of
cut subproblem). If a subproblem corresponding to vw-cut
C separated by Algorithm 18 is added to the subproblem set
S, the guaranteed improvement of the global lower bound is
0C
C
|}. Where Œ∏C is the reparametrized
, |Œ∏vw
min{minuv‚ààC Œ∏uv
cost used for the cut factor initialization.
Proof. We obtain the reparametrized cost Œ∏C for the cut
subproblem analogically as in Formula 18 for the path subproblem. Note that Algorithm 18 ensures that all cut edges
in the separated cut subproblem have positive cost and the
lifted edge vw has negative cost. Using the same arguments
as in the proof of Proposition 3, we obtain the lower bound
change of inflow and outflow factors after separating the cut
subproblem:
‚àÜLout + ‚àÜLin =
X
out
‚àí
œâuv ¬∑ Œ≥uv
‚àí
out <0
uv‚ààC:Œ≥uv

X

(24)

0
in
œâuv
¬∑ Œ≥uv

in <0
uv‚ààC:Œ≥uv

0
0out
0
0in
‚àí œâvw
Œ≥vw
‚àí œâvw
Œ≥vw
‚â•
(23)
X
out
in
0
0out
‚àí
œâuv ¬∑ (Œ≥uv + Œ≥uv ) ‚àí œâvw Œ≥vw
out +Œ≥ in <0
uv‚ààC:Œ≥uv
uv

0
0in
0
0out
0
0in
0C
‚àí œâvw
Œ≥vw
= ‚àíœâvw
Œ≥vw
‚àí œâvw
Œ≥vw
= ‚àíŒ∏vw

19

Algorithm 19 Message Passing
Input Graphs G = (V, E) and G0 = (V, E 0 ), costs Œ∏ ‚àà
0
RV ‚à™E‚à™E
Output Best found primal solution (z, y, y 0 )ub , lower
bound LB
1: Initialization:
2: for v ‚àà V do
3:
Add inflow subproblem for node v.
4:
(
Œ∏uv
if v = s
‚àí
in
‚àÄuv ‚àà Œ¥E (v) : Œ∏uv = 1
Œ∏
otherwise
2 uv
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:

Algorithm 20 Inflow-Subproblem-Message-Passing
Input central vertex u of the subproblem
1: Œ≥ 0in = All-Lifted-MM-In(u, Œ∏ in ).
2: œâ = 1
‚àí
3: for vu ‚àà Œ¥E (u), P ‚àà P : vu ‚àà PE do
P
4:
Œ≥vu = Path-Base-Min-Marginal(u, v, Œ∏in )
5:
œâ += 1
6: end for
‚àí
7: for vu ‚àà Œ¥E 0 (u), P ‚àà P : vu ‚àà PE 0 do
0P
8:
Œ≥vu = Path-Lifted-Min-Marginal(u, v, Œ∏in )
9:
œâ += 1
10: end for
‚àí
11: for vu ‚àà Œ¥E 0 (u), P ‚àà P : P ‚àà vu-paths(G) do
0P
12:
Œ≥vu = Path-Lifted-Min-Marginal(u, v, Œ∏in )
13:
œâ += 1
14: end for
‚àí
15: for vu ‚àà Œ¥E (u), C ‚àà C : vu ‚àà CE do
C
16:
Œ≥vu = Cut-Base-Min-Marginal(u, v, Œ∏in )
17:
œâ += 1
18: end for
‚àí
19: for vu ‚àà Œ¥E 0 (u), C ‚àà C : C is a vu-Cut do
0C
= Cut-Lifted-Min-Marginal(u, v, Œ∏in )
20:
Œ≥vu
21:
œâ += 1
22: end for
‚àí
23: for vu ‚àà Œ¥E 0 (u) do
out
0in
0in
in
+= œâ1 ¬∑ Œ≥vu
, Œ∏vu
‚àí= œâ1 ¬∑ Œ≥vu
24:
Œ∏vu
25: end for
‚àí
26: for vu ‚àà Œ¥E (u), P ‚àà P : vu ‚àà PE do
in
P
P
P
27:
Œ∏vu
‚àí= œâ1 ¬∑ Œ≥vu
, Œ∏vu
+= œâ1 ¬∑ Œ≥vu
28: end for
‚àí
29: for vu ‚àà Œ¥E 0 (u), P ‚àà P : vu ‚àà PE 0 do
0in
0P
0P
0P
‚àí= œâ1 ¬∑ Œ≥vu
+= œâ1 ¬∑ Œ≥vu
30:
Œ∏vu
, Œ∏vu
31: end for
‚àí
32: for vu ‚àà Œ¥E 0 (u), P ‚àà P : P ‚àà vu-paths(G) do
0P
0P
0P
0in
, Œ∏vu
+= œâ1 ¬∑ Œ≥vu
33:
Œ∏vu ‚àí= œâ1 ¬∑ Œ≥vu
34: end for
‚àí
35: for vu ‚àà Œ¥E (u), C ‚àà C : vu ‚àà CE do
in
C
C
C
36:
Œ∏vu ‚àí= œâ1 ¬∑ Œ≥vu
, Œ∏vu
+= œâ1 ¬∑ Œ≥vu
37: end for
‚àí
38: for vu ‚àà Œ¥E 0 (u), C ‚àà C : C is a vu-Cut do
0in
0C
0C
0C
39:
Œ∏vu ‚àí= œâ1 ¬∑ Œ≥vu
, Œ∏vu
+= œâ1 ¬∑ Œ≥vu
40: end for

‚àí
1 0
0in
‚àÄuv ‚àà Œ¥E
0 (v): Œ∏uv =
2 Œ∏uv .
1
Œ∏vin = 2 Œ∏v .
Add outflow subproblem for node v with analoguous costs.
end for
C=‚àÖ
P=‚àÖ
Lagrange decomposition optimization
for iter = 1, . . . , max iter do
Forward Pass:
for u = u1 , . . . , u|V | do
Inflow-Subproblem-Message-Passing(u)
Outflow-Subproblem-Message-Passing(u)
end for
for P ‚àà P do
Path-Subproblem-Message-Passing(P )
end for
for C ‚àà C do
Cut-Subproblem-Message-Passing(C)
end for
Backward Pass:
Revert order of nodes and perform above iteration.
if iter ‚â° 0 mod k then
Separate-Cut-Subproblem(Œµ)
Separate-Path-Subproblems(Œµ)
Add cut and path subproblems to C and P
end if
if iter ‚â° 0 mod l then
(z, y, y 0 ) =Compute-Primal(S, Œ∏)
if hŒ∏, (z, y, y 0 )i < hŒ∏, (z, y, y 0 )ub i then
(z, y, y 0 )ub = (z, y, y 0 )
end if
end if
LB =Lower-Bound
end for

20

Algorithm 21 Path-Subproblem-Message-Passing
Input: uv-Path P ‚àà P
1: Œ≥ P = Path-Min-Marginals(P, Œ∏ P )
1
2: œâ P = 2¬∑|P |+2¬∑|P
E
E0 |
3: for kl ‚àà PE do
P
P
4:
Œ∏kl
‚àí= Œ≥kl
in
P
out
P
5:
Œ∏kl ‚àí= œâ P ¬∑ Œ≥kl
, Œ∏kl
‚àí= œâ P ¬∑ Œ≥kl
6: end for
7: for kl ‚àà PE 0 do
P
P
8:
Œ∏kl
‚àí= Œ≥kl
in
P
out
P
9:
Œ∏kl ‚àí= œâ P ¬∑ Œ≥kl
, Œ∏kl
‚àí= œâ P ¬∑ Œ≥kl
10: end for

objective function.
For each vertex of each path, function split evaluates the
cost of splitting the path after the vertex:
‚àÄvj ‚àà PV = (v1 , . . . vn ) :
(25)
X
split(vj , P ) = ‚àí
Œ∏v0 k vl ‚àí Œ∏vj vj+1 + Œ∏svj+1 + Œ∏vj t
k‚â§j,l>j,
vk vl ‚ààE 0

The second step of the primal solution post-processing
by Algorithm 25 is merging paths. Before the path merging
itself, some candidate pairs of paths need to be shortened at
their ends in order to enable their feasible merging.
Algorithm 26 identifies pairs of those paths whose merging should lead to objective improvement but that cannot be
connected directly due to missing base edge between their
endpoints. In order to identify the desired paths pairs, several functions are used.
Function l+ (P1 , P2 ) resp l‚àí (P1 , P2 ) is the sum of positive resp. negative lifted edges from path P1 to path P2 .
Function l(P1 , P2 ) sums all lifted edges from P1 to P2 .

Algorithm 22 Cut-Subproblem-Message-Passing
Input: uv-Cut C ‚àà C
1: Œ≥ C = Cut-Min-Marginals(C, Œ∏ C )
2: œâ C = 2¬∑|C1 |+2
E
3: for kl ‚àà CE do
C
C
4:
Œ∏kl
‚àí= 2œâ C ¬∑ Œ≥kl
C
C
out
C
in
+= œâ C ¬∑ Œ≥kl
5:
Œ∏kl += œâ ¬∑ Œ≥kl , Œ∏kl
6: end for
0C
0C
‚àí= 2œâ C ¬∑ Œ≥uv
7: Œ∏uv
0C
out
in
C
0C
+= œâ C ¬∑ Œ≥uv
Œ∏uv
8: Œ∏uv += œâ ¬∑ Œ≥uv ,

‚àÄP1 , P2 ‚àà P
l+ (P1 , P2 ) =

X
uv‚ààE 0 :u‚ààP

l‚àí (P1 , P2 ) =

Algorithm 23 Lower-Bound
Input Subproblems S
Output Lower bound value LB
1: LB = 0
2: for u ‚àà V \{s, t} do
3:
LB += Opt-In-Cost(u, Œ∏in )
4:
LB += Opt-Out-Cost(u, Œ∏out )
5: end for
6: for P ‚àà P do
7:
LB += Path-Subproblem-Optimization(P, Œ∏P )
8: end for
9: for C ‚àà C do
10:
LB += Cut-Subproblem-Optimization(C, Œ∏C )
11: end for

0
Œ∏uv

0
1 ,v‚ààP2 ,Œ∏uv‚â•0

X

0
Œ∏uv

(26)

0
uv‚ààE 0 :u‚ààP1 ,v‚ààP2 ,Œ∏uv<0

l(P1 , P2 ) = l+ (P1 , P2 ) + l‚àí (P1 , P2 )
We use the above values in functions merge and mergeœÑ
that evaluate the gain of merging two paths. Threshold œÑ ‚â§
1 constraints the ratio between the positive and the negative
part of lifted cost function l that is considered acceptable for
merging two paths.
‚àÄP1 =(v1 , . . . , vn ), P2 = (u1 , . . . , um ) ‚àà P
(27)
(
Œ∏vn u1 + l(P1 , P2 ) if vn u1 ‚àà E
merge(P1 , P2 ) =
‚àû
otherwise

‚àÄP1 = (v1 , . . . , vn ), P2 = (u1 , . . . , um ) ‚àà P

8.8. Primal Solution and Local Search

mergeœÑ (P1 , P2 ) =
(28)
Ô£±
Ô£¥
if vn u1 ‚àà
/E‚à®
Ô£≤‚àû
+
=
l (P1 , P2 ) > œÑ |l‚àí (P1 , P2 )|
Ô£¥
Ô£≥
Œ∏vn u1 + l(P1 , P2 ) otherwise

Algorithm 24 summarizes the whole procedure for obtaining a primal solution. As stated in Section 4.6, we obtain an initial primal solution by solving MCF problem.
Given a feasible solution of the LDP, Algorithm 25 improves it by splitting and merging paths. While we obtain
the costs for MCF from base and lifted edges costs in inflow
and outflow factors (Algorithm 4), the local search procedure uses original input costs of base and lifted edges.
Algorithm 28 finds candidate split point of each path and
recursively splits the path if the split leads to decrease of the

Algorithm 27 is applied on all paths pairs found by Algorithm 26. It inspects whether shortening of one or both paths
leads to a feasible connection that ensures a desired objective improvement. It iteratively removes either the last vertex of the first path or the first vertex of the second path and
checks if a connection is possible and how much it costs.
21

The last part of Algorithm 25 considers merging paths.
We use formula mergeœÑ (Pi , Pj )‚àíout(Pi )‚àíin(Pj ) to evaluate whether merging two paths is beneficial. Here in(Pj )
denotes input cost to the first vertex of Pj and out(Pi ) denotes output cost from the last vertex of Pi . We state the
full formula just for completeness. We set the input and the
output costs to zeros in our experiments. Using mergeœÑ ensures that we connect the paths only if the ratio between the
positive lifted cost l+ and negative lifted cost l‚àí between
the paths is below the acceptable threshold.

Algorithm 26 Shorten-For-Merge
Input Set of paths P
Output Updated set of paths P
1: for all P1 = (v1 , . . . , vn ) ‚àà P do
2:
P =
argmin
merge(P1 , P2 )

Algorithm 24 Compute-Primal

10:
11:
12:
13:
14:
15:

P2 =(u1 ,...,um )‚ààP:vn u1 ‚ààE

P0 =

4:

if

5:
6:
7:
8:
9:

|V 0 |+|E|+|E 0 |

Input Subproblems S, original costs Œ∏ ‚àà R
Output Primal solution (z, y, y 0 )
1: Init-MCF
mcf
2: Obtain primal solution of MCF y mcf ‚àà {0, 1}E
mcf
3: Set (z, y) accroding to y
4: y 0 =Adjust-Lifted-Solution(z, y)
5: (z, y, y 0 ) =Local-Search(z, y, y 0 )

16:
17:

argmin
l(P1 , P2 )
P2 =(u1 ,...,um )‚ààP:vn u1 ‚ààE
/
l(P1 , P 0 ) < merge(P1 , P ) ‚àß l(P1 , P 0 )
P ‚àó = P 0 , c = l(P1 , P 0 )

7:

argmin mergeœÑ (Pi , Pj )
(Pi ,Pj )‚ààP√óP

‚àí out(Pi ) ‚àí in(Pj )
if mergeœÑ (P1 , P2 ) ‚àí out(P1 ) ‚àí in(P2 ) < 0 then
P = Merge-Paths(P1 , P2 , P)
else
break
12:
end if
13: end while
14: (z, y, y 0 )=Set-From-Paths(P)

8:
9:
10:
11:

22

< 0 then

else
P ‚àó = P , c = merge(P1 , P )
end if
if pred[P ‚àó ] = ‚àÖ ‚à® score[P ‚àó ] > c then
pred[P ‚àó ] = P1 , score[P ‚àó ] = c
end if
end for
for all P2 = (u1 , . . . , um ) ‚àà P do
if pred[P2 ]=P1 = (v1 , . . . , vn ) ‚àß vn u1 ‚àà
/ E then
P =Cut-Ends(P1 , P2 , P)
end if
end for

Algorithm 28 Check-Path-Split
Input Input path P , set of all paths P
Output Set of paths P
1: vm = argmaxvj ‚ààPV split(vj , P )
2: if split(vm , P ) < 0 then
3:
(P1 , P2 ) =Split-Path(P, vm )
4:
P.remove(P ), P.insert(P1 ), P.insert(P2 )
5:
P =Check-Path-Split(P1 , P)
6:
P =Check-Path-Split(P2 , P)
7: end if
8: return P

Algorithm 25 Local-Search
Input Input primal solution z, y, y 0
Output Improved primal solution z, y, y 0
1: Obtain set of disjoint paths P = {P1 , . . . , Pn } from y
2: for all P ‚àà P do
3:
P =Check-Path-Split(Pi , P)
4: end for
5: P =Shorten-For-Merge(P)
6: while true do
(P1 , P2 ) =

3:

Algorithm 27 Cut-Ends
Input P1 = (v1 , . . . , vm ), P2 = (u1 , . . . , um ), P, imax
Output New set of paths P
1: c1 = ‚àû, c2 = ‚àû
2: while i1 + i2 < imax do
3:
P10 = (v1 , . . . , vn‚àíi1 ), P20 = (u1+i2 , . . . , um )
4:
P100 = (v1 , . . . , vn‚àíi1 ‚àí1 ), P200 = (u2+i2 , . . . , um )
5:
if merge(P10 , P200 ) + merge(P100 , P20 ) < ‚àû then
6:
Œ±1 = merge(P10 , P200 ) + split(P1 , vn‚àíi1 ) +
split(P2 , u1+i2 )
7:
Œ±2 = merge(P100 , P20 ) + split(P1 , vn‚àíi1 ‚àí1 ) +
split(P2 , ui2 )
8:
if Œ±1 < Œ±2 then c1 = i1 ‚àí 1, c2 = i2
9:
else c1 = i1 , c2 = i2 ‚àí 1
10:
break
11:
else if merge(P10 , P200 ) < ‚àû then
12:
c1 = i1 ‚àí 1, c2 = i2
13:
break
14:
else if merge(P100 , P20 ) < ‚àû then
15:
c1 = i1 , c2 = i2 ‚àí 1
16:
break
17:
else
18:
Œ±1 = l(P10 , P200 ) + split(P1 , vn‚àíi1 )+
split(P2 , u1+i2 )
19:
Œ±2 = l(P100 , P20 ) + split(P1 , vn‚àíi1 ‚àí1 )+
split(P2 , ui2 )
20:
if Œ±1 < Œ±2 then i2 ++
21:
else i1 ++
22:
end if
23: end while
24: if c1 6= ‚àû ‚àß c2 6= ‚àû then
25:
P10 = (v1 , . . . , vn‚àíc1 ), P20 = (u1+c2 , . . . , um )
26:
if mergeœÑ (P10 , P20 ) < ‚àû then
27:
if c1 > 0 then
28:
(P11 , P12 ) =Split-Path(P1 , vn‚àíc1 )
29:
P.remove(P1 )
30:
P.insert(P11 ), P.insert(P12 )
31:
end if
32:
if c2 > 0 then
33:
(P21 , P22 ) =Split-Path(P2 , uc2 )
34:
P.remove(P2 )
35:
P.insert(P21 ), P.insert(P22 )
36:
end if
37:
end if
38: end if
39: return P

Global context normalization puts similarity measurements into global perspective to form more meaningful feature values. For instance, global illumination changes will
likely decrease measured appearance similarities between
any pair of detections. Likewise, a scene where all people are far away from the camera will most likely result in
less confident appearance similarity measurements. In both
cases, positive matching pairs should have higher similarities than negative matching pairs but the absolute similarity
values are reduced by the global effects. These and further
effects make the interpretation of the similarity in absolute
terms less meaningful. Global context normalization compensates such effects.
To this end, let ‚Ñ¶k = {œÉvw,k : vw ‚àà E} comprise all computed similarity measurements for feature k ‚àà
{Spa, App} defined in Section 5.1. For each feature k
and similarity measurement œÉvw,k ‚àà ‚Ñ¶k , we define sets
GCi,k with i ‚àà [5]. Each set GCi,k induces two global
context normalization features: œÉvw,k ¬∑ max(GCi,k )‚àí1 and
2
œÉvw,k
¬∑ max(GCi,k )‚àí1 .
The sets are defined w. r. t. œÉvw,k ‚àà ‚Ñ¶k as follows:
GC1,k = {svn,k ‚àà ‚Ñ¶k : n ‚àà B} .

(29)

GC2,k = {smw,k ‚àà ‚Ñ¶k : m ‚àà B} .

(30)

GC3,k = {svn,k ‚àà ‚Ñ¶k : n ‚àà B and fn = fw } .

(31)

GC4,k = {snw,k ‚àà ‚Ñ¶k : n ‚àà B and fn = fv } .

(32)

GC5,k = {smn,k ‚àà ‚Ñ¶k : m, n ‚àà B} .

(33)

where fx denotes the frame of detection x and B the batch
as defined in section 5.1. The sets GC1,k and GC2,k result
in a normalization of the similarity score œÉvw,k over all outgoing or incoming edges to v or w, respectively. The set
GC3,k results in a normalization over all similarity scores
for outgoing edges from v to a detection in frame fw . Analogously, the set GC4,k collects all edges from a detection
of frame fv to node w. Finally, GC5,k normalizes the similarity score over all existing scores in the batch B.

8.10. Multi Layer Perceptron (MLP)
As reported in Section 5.1 we use a lightweight and scalable MLP to obtain edge costs. We use multiple instances
of the same MLP structure. Each MLP is trained on edges
that have a specific range of temporal gaps (more details in
Section 5.1).
The MLP architecture is based on two fully connected
(FC) layers. The input is a 22-dimensional vector (features with corresponding global context normalizations).
LeakyReLU activation [43] is used for the first FC layer.
The final layer (FC) has one neuron, whose output represents the cost value. For training, an additional sigmoid
activation is added. The structure of the neural network is
visualized in Figure 3.

8.9. Global Context Normalization
Our tracking system employs a global context normalization to obtain accurate features between detections (see
Section 5.1). This section elaborates the implementation
details.
23

and for the entire sequence can then be roughly estimated4
with 63 ¬∑ 106 and 4 ¬∑ 109 , respectively, which is intractable.
To decrease the amount of edges per batch while ensuring that batches consists of samples containing all permissible temporal gaps, we adapt batch creation to our needs: For
each start frame f of a batch, we subselect the frames to be
considered within the range f, . . . , fmax . During training,
we then subsample detections from these frames. During
inference, we utilize all detections of these frames to form
our batch.
To this end, we define a sequence of frame shifts that
is used to create the frame subselection. Using only few
frame shift makes the approach more computationally efficient. Yet we must ensure that all edges are computed at
least once during inference. That is if B(f ) denotes the
batch created
according to our strategy with starting frame
S
f , then f B(f ) = E must contain all edges.
To ensure that we always cover temporal gaps of up to 2
seconds, the frame shifts depend on the maximal permissible temporal gaps (measured in frames).
For a start frame f and ‚àÜfmax = 50, we define the frame
shift set Sh(‚àÜfmax ) as

Input Features
(22)
FC
(22)
LReLU

(22)
FC

(1)
Cost Value

Figure 3. Visualisation of the proposed neural MLP. FC denotes
fully connected layers, LReLU denotes LeakyReLU activation
[43] and values in parenthesis denote the dimension of the corresponding tensors.

8.11. Batch Creation Using Fixed Frame Shifts
In this section, we elaborate on our batch creation
method using fixed frame shifts (see Paragraph Training of
Section 5.1).

Sh(50) = {0, 1, 2, 3, 4, 5, 6, 7, 8, 17, 26, 35, 44, 50} . (34)

A batch B contains a set of edges with corresponding
edge costs. Important to note is that we use the same batch
creation strategy to form batches for training as well as inference. During training, batches are augmented with corresponding ground truth labels. A carefully chosen batch
creation strategy is crucial to ensure accurate and scalable
training and inference.

For ‚àÜfmax = 60, we define the frame shift set Sh(‚àÜfmax )
as
Sh(60) = { 0, 1, 2, 3, 4, 5, 6, 7, 14, 21, 28, 36, 37,
38, 40, 46, 53, 60 }

(35)

Then for each start frame f , a batch is created using the
frames {f + fshift : fshift ‚àà Sh(fmax )}. To ensure
that all edges are computed at least once in the inference stage, one need to calculate batches with start frames
f ‚àà {1 ‚àí ‚àÜfmax , . . . , ‚àÜfmax }. Compared to the naƒ±Ãàve
batch creation
strategy, our utilized batch creation results
P13
in 226 i=1 226 ¬∑ (14 ‚àí i) = 4647916 edges for MOT2020, using the same assumptions as before. The number of
edges to be computed is thus significantly lower.

In order to obtain accurate predictions by our MLPs
(Section 5.1), the distribution of a batch should represent
the characteristics of the training data. In particular, a batch
should comprise edges covering all permissible temporal
gaps between detections. Furthermore, the distribution of a
batch influences the costs of all edges contained in the batch
by the global context normalization (Section 8.9). Thus also
during inference, a batch should comprise edges covering
all permissible temporal gaps between detections. It is thus
important to employ the same batch creation strategy for
training and inference.

8.12. Determining obviously matching and nonmatching detection pairs

A naƒ±Ãàve strategy is thus to define a batch on all frames
within a range f up to f +‚àÜfmax , where f is a starting frame
and ‚àÜfmax defines the maximal permissible time gap. During training, one could then sample detections from these
frames and randomly create true positive and false positive
edges. However, such an approach is not tractable during
inference for long sequences with many detections and long
time gaps. To see this, consider the sequence MOT20-05 of
the MOT20 dataset [16]. It contains 3315 frames with 226
detections per frame on average. We use a maximal permissible time gap of 2 seconds, which correspond to 50 frames
for sequence MOT20-05. The number of edges per batch

During the graph construction in Section 5.2, we employ
a simply strategy to detect edges that represent obviously
matching or obviously non-matching detection pairs. Corresponding edge costs are set such that they induce mustlinks or cannot-links as soft constraints. Details are described in this section.
Obviously non-matching pairs. We use optical flow
and the object size to calculate the maximal plausible
P
226 detections per frame, there are about 226 49
i=1 226(50 ‚àí
i) = 62568100 edges per batch. With 3315 frames there are more
than 3315
62568100 = 4148265030 edges. Here we assumed non50
overlapping batches, thereby missing many connections.
4 With

24

Table 4. Results with and w/o post-processing on MOT20 train set.

displacement dmax (v, w) and velocities vx,max (v, w) and
vy,max (v, w) between two detections v and w. If v is a detection in frame fv and w a detection in fw , we define
dmax (v, w) = kd +

fX
w ‚àí1

max(O(i, i + 1))

w
w/o

MOTA‚Üë

IDF1‚Üë

TP‚Üë

FP‚Üì

FN ‚Üì

IDS ‚Üì

74.4
72.3

62.8
63.6

863203
833473

15778
8462

271411
301141

3511
4201

(36)

i=fv

edge the mean magnitude given by the optical flow between
the frames of the respective detections. Before we compute
the intersection over union, we translate one of the boxes in
horizontal direction by the approximated camera motion, if
this decreases the intersection over union. This procedure
lowers the likelihood of creating wrong must-links caused
by camera motion. Camera motion compensation needs to
be applied only to sequences filmed from a non-static camera. Optical flow can be used to detect if a scene has a
static camera setup. Note that MOT20 contains only scenes
filmed from a static camera.

where max(O(i, i + 1)) is the maximal magnitude of the
optical flow between the frames i and i + 1 and kd is a
security tolerance, which we set to 175 pixel according to
experiments on the training data. If the distance d(v, w) between the center points of detections v and w is greater than
dmax (v, w), the detection pair given by vw is regarded as
obviously non-matching. We also assume that the maximal
velocity of a person is limited. With the height h and width
b of the bounding boxes, we define the maximal velocities
vx,max (v, w) = b ¬∑ ke

(37)

and

8.13. Inference

ke
(38)
2
in x and y-direction. The factor ke is set to ke = 0.3 according to experiments on the training data. In sequences
with moving cameras, the factor is increased to ke = 0.8
and decreased to ke = 0.12 in sequences with static camera
and aerial viewpoint. If the velocity vx (v, w) or vy (v, w)
between the detections v and w is greater than corresponding vx,max (v, w) or vy,max (v, w), the connection given by
vw is regarded as obviously non-matching. To avoid wrong
interpretations caused by noise in the velocity calculation,
we set ke to a high value for detection pairs with small temporal distances.
If a detection pair vw is regarded as obviously nonmatching, we induce a cannot-link soft constraint on vw
by setting its costs to a negative value with a high absolute
value, i.e. cvw  0.
Obviously matching pairs. We induce must-link soft constraints on edges, considering only connections between
consecutive frames.
An edge vw ‚àà E with an appearance similarity score
œÉvw,App close to the maximal achievable score (which is 2
in our implementation) is considered an obviously matching
pair. We infer from the training data ks = 1.95 as threshold,
so that edges with œÉvw,App > ks are regarded as obviously
matching by setting their costs accordingly.
In addition, if two boxes between consecutive frames
have a high overlap, we induce a must-link soft constraint
on the corresponding edge. In more detail, the intersection over union between the detections must be at least 0.5.
However, such spatial measurements are affected by camera motions, thus potentially leading to wrong interpretations. In order to induce link soft-constraints only in confident cases, we employ a simple camera motion compensation beforehand. To this end, we calculate for a considered
vy,max (v, w) = h ¬∑

8.13.1

Interval Solution

This section explains how we solve MOT20 using solutions of its intervals. First, we solve the problem in independent subgraphs containing detections and edges from
time intervals [il + 1, (i + 1)l] for i ‚àà {0, 1 . . . , n}, where
l = 3 ¬∑ tmax , and tmax is the maximum temporal edge
length. We fix resulting trajectories in the centres of intervals, namely in time intervals [il +tmax +1, (i+1)l ‚àítmax ]
for i ‚àà {1, . . . , n ‚àí 1}. Second, we solve the problem in
time intervals covering the end of one initial interval and the
beginning of the subsequent interval while allowing connections with the fixed trajectory fragments. The cost of a
connection between a detection and a trajectory fragment is
obtained as the sum of costs between the unassigned detection and the detections within the trajectory fragment.
8.13.2

Post-Processing

We perform post-processing on the result provided by our
solver. As it is common, we recover missing detections
within a computed trajectory using linear interpolation. We
also correct wrong connections that mostly stem from situations which currently cannot be correctly resolved by current features, independent of the tracking system, e.g. pairwise features computed over very long temporal gaps and
ambiguous feature information due to multiple people appearing within one detection box.
Consequently, we apply these strategies only to MOT20.
As soon as one of these methods detects a connection as
false, the corresponding trajectory is split into two new trajectories. Table 4 shows the effect of the post-processing on
MOT20 train set.

25

8.14.1

We noticed an accumulation of wrong connections, where
one end of a trajectory (i.e. its first or last detection) is connected to the successive detection using a skip-connection
over a long temporal gap, and the connection is wrong. This
might be explained by a combination of misleading visual
features (e.g. caused by partial occlusion), not very informative spatial features (due to the high temporal gap) and
missing lifted edges, because only one detection is existent
at the end of the trajectory. To keep only reliable connections, we split trajectories if only one detection is existing at
the start or end of a trajectory, followed by a temporal gap
of at least 10 frames.
We also handle cases at the borders of a tracking scene.
If a person leaves the scene, and another person enters the
scene at a position close by after a short time, the tracking
system sometimes joins the trajectories of the two persons.
We explain this behaviour by the high visual similarity between partially visual persons at image borders. If a person
leaves a scene, normally just one leg, one arm or the head is
visible for some frames. However, a single body party is not
very discriminative and thus can look similar to a body part
of another person. In addition, the spatio-temporal information will indicate a likely match in this scenario. Due to the
temporal gap, no or not many meaningful lifted edges are
existing which could give contradicting signals. To eliminate this kind of errors, we split trajectories between two
detections, if the temporal gap is greater or equal to 10 and
both detections are located at the image border.
For all detections which are connected over a temporal
time gap greater or equal to 10 frames (skip edges), we perform a motion sanity check and split the corresponding trajectory if its motion is not plausible. To this end, we first
determine the highest velocity of obviously correct trajectories, or parts of trajectories with a minimal length of 10
frames (to avoid random noise issues). Then, we split connections at these skip edges, if their velocity is higher than
the determined velocity.
Additionally, we verify that motion between trajectory
parts are consistent. To this end, we consider the motion described by a trajectory, using the part before a connection,
and compare it with the resulting motion described by the
trajectory, using the part after the connection. If the velocities differ by a factor of 5 or greater or if the angle differs
more than œÄ/2, the trajectory is split.

Computational Complexity

The solver terminates if one of the following conditions is
satisfied. Either the lower bound is equal to the objective
value of the best primal solution, i.e. optimum has been
found. Or the maximum number of message passing iterations has been reached. The optimum was not found in our
experiments, so the letter condition applied.
The runtime of the solver is, therefore, determined by
the input parameter denoting the maximum number of iterations. The dependence on number of iterations is not exactly linear because the problem size grows with the number
of path and cut subproblems added to set of subproblems S
via cutting plane separation (see Sections 8.4 and 8.5).
An overview of the whole solver run and the tasks performed within one its iteration is given in Section 8.7. The
runtime of the tasks is given by the runtime of computing
min-marginals of the subproblems.
We discuss the complexity of the used algorithms in the
paragraphs bellow. They all have a polynomial complexity.
Therefore, the overall runtime of the solver is polynomial
too.
In order to compute messages between the inflow and
the outflow subproblem, we apply Algorithm 6. Minmarginals for messages between the path subproblems and
the in/outflow subproblems are obtained for one shared variable at the time. The same holds for exchanging messages
between the cut subproblems and the in/outflow subproblems. This is done by calling restricted versions of optimization algorithms of the path and cut subproblems (Algorithms 12 and 3). For in/outflow subproblems, we use
one call of Algorithm 1 followed by either Algorithm 9 or
Algorithm 11 limited to single variable reparametrization.
Messages between inflow and outflow subproblems.
Messages between inflow and outflow subproblems are realized on lifted edge variables by calling Algorithm 6.
Many subroutines employ full or partial DFS on all nodes
reachable from the central node within the relevant time
gap. In these cases, we use precomputed node order instead of complete DFS as described in the last paragraph
of Section 8.2. One call of the full DFS (Algorithms 1
and 11) requires to process all vertices reachable from v
within maximal time gap for edge length (‚àÜfmax ). This
comprises Lmax video frames (we use Lmax = 50 or 60).
Let us denote by n the maximum number of detections in
one frame. The complete DFS processes maximally nLmax
vertices. Incomplete DFS used in Algorithm 9 processes
in each step vertices in L layers. In the worst case, this
is done for all relevant layers L = 1, . . . , Lmax . Processing one vertex requires to check its neighbors in the base
graph. Their amount is bounded by KLmax where K = 3.
See Sparsification paragraph in Section 5. Putting all together, the complexity of Algorithm 6 for one subproblem
is O(nL3max ). We have two subproblems for each (lifted)

8.14. Solver Runtime
Our solver can compute one interval of MOT20 (150
frames) or an entire sequence of MOT17 with less than
20GB RAM, using a single CPU core.
Subsequently, we analyze the runtime in detail, by performing a theoretical analysis in Section 8.14.1, followed by
a comparison with an existing LDP solver in Section 8.14.2.
26

graph vertex, yielding complexity O(|V 0 |nL3max ) for sending messages between all inflow and outflow subproblems
in one message passing iteration.
Messages from path subproblems.
Obtaining min
marginal for one edge variable of a path subproblem requires two calls of restricted Algorithm 12 whose complexity is linear in the number of path edges. So, min-marginals
for all path edges are obtained in O(|P |2 ).
Messages from cut subproblems.
Min marginal of
one variable of a cut subproblems is obtained by adjusting
its optimization Algorithm 3. The complexity is given by
the complexity of the employed linear assignment problem
which can be solved in polynomial time.
Cutting plane procedures. The cutting plane algorithms
are called each 20 iterations. We allow to add maximally
0.5 ¬∑ |S0 | new factors during one separation call, where S0
is the initial set of subproblems containing only inflow and
outflow factors. So it holds, |S0 | = 2|V 0 |. Once added,
the subproblems influence the runtime via taking part in
the message passing (see Section 8.7). Cutting plane itself
(Sections 8.4 and 8.5) contains sorting of subsets of base or
lifted edges which has complexity O(|E ‚àí | log |E ‚àí |) (resp.
O(|E + | log |E + |)). The other algorithms run in quadratic
time w.r.t. number of vertices within relevant time distance
to the currently processed edge.
Primal solution. We compute new primal solution in each
five iterations. We use Algorithm 4 for obtaining base edge
costs. Then, we use successive shortest paths algorithm
for solving minimum cost flow problem and finally local
search heuristic given by Algorithm 25, see Section 4.6.
The complexity of solving MCF is the complexity of successive shortest path algorithm which is polynomial. Local
search heuristic requires to compute and update cummulative costs between candidate paths. They can be computed
in time linear in the number of lifted edges O(|E 0 |). MCF
costs are obtained by calling Algorithm 1. Its complexity is
discussed above.
8.14.2

with LifT using both the two-step procedure and the global
solution.
Influence of input costs. Our input data contain many
soft constraints for obviously matching pairs of detections.
Those are edges with negative costs significantly higher in
absolute value than other edges costs. LifT finds an initial
feasible solution using only base edges. This solution may
be already very good due to the costs of obviously matching
pairs. Moreover, Gurobi contains a lot of efficient precomputing steps, so it can recognize that the respective variables should be active in the optimum and reduce the search
space.
Parameters. We adjust parameters of our solver to work
with comparable data as LifT. For instance, we do not set
cost of any base edges to zero (as described in Section 5.2)
because LifT does not enable this option. So, the costs
of overlapping base and lifted edges are duplicated as opposed to the most of other experiments. Moreover, if there
is no edge between two detections within the maximal time
distance in the input data, we can add a lifted edge with
high positive cost for such pair in ApLift. This is useful
for reducing the input size for MOT20 dataset. LifT does
not have this option. Therefore, we disable this option for
ApLift too.
Subsequences of MOT20. We present a comparison between our solver and LifT using two-step procedure on an
example subsequence of MOT20-01 in Table 3 in the main
text. On that subsequence, our solver is faster and has even
slightly better IDF1 score than LifT. In Table 5, we present a
comparison on first n frames of sequence MOT20-02 where
LifT finds solutions faster than our solver using many iterations. We assume that this is caused by the input costs that
are convenient for Gurobi, see the discussion above.
Train set of MOT17. We compare our solver with LifT on
global training sequences of MOT17. That is, we do not use
two-step procedure. Therefore, LifT finds the globally optimal solution if it finishes successfully. The runtime of LifT
is exponential in general and it can be often killed because
of memory consumption if run on global sequences. Therefore, we perform these experiments on a machine having
2000 GB RAM and multiple CPUs each having 64 cores.
The results are in Table 6. Asterisk in LifT time column indicate that the problem cannot be finished. Some of
the processes are killed by the system because of too much
memory consumption. Some processes do not finish within
more than 27 hours. Moreover, LifT often occupied up to
30 cores for solving one sequence. Our solver uses only
one core. In the cases when LifT does not finish, we evaluate the best feasible solution found by LifT. Those were
typically the initial feasible solutions. That is, the solutions
that ignore the lifted edges. Obtaining the initial solutions
for these difficult instances took between 1700 and 4600
seconds. The numbers in brackets relate our results to LifT

Comparison with LifT

We perform several experiments for comparing our solver
with an optimal solver for lifted disjoint paths LifT [28].
LifT global solution vs. two-step procedure. LifT is
based on ILP solver Gurobi. It solves the LDP problem
optimally. However, it is often not able to solve the problem on the full graphs. Therefore, LifT uses a two-step
procedure. First, solutions are found on small time intervals to create tracklets. Second, the problem is solved on
tracklets. This approach simplifies the problem significantly
but the delivered solutions are not globally optimal anymore. We have observed that using our input costs, LifT
is able to solve some problem sequences globally without
the two step-procedure. Therefore, we compare our solver
27

8.16. Tracking Metrics

results. The time column provides the ratio between our
time and LifT time. The IDF1 column presents the difference between ApLift and LifT.

A detailed evaluation of our proposed MOT system in
terms of tracking metrics for all sequences of the datasets
MOT20 [16] and MOT17 [45] are reported in Table 7. Evaluations on the test set are performed by the official benchmark evaluation server at https://motchallenge.net where
our test results are reported as well. The tracking method
for training sequences are trained with leave-one-out strategy to avoid overfitting on the corresponding training sequence.

Table 5. Runtime and IDF1 comparison of LDP solvers: ApLift
(ours) with 6, 11, 31 and 51 iterations and LifT[28] (two step procedure) on first n frames of sequence MOT20-01 from MOT20.
n

Measure

LifT

Our6

Our11

Our31

Our51

50

IDF1‚Üë
time [s]

83.4
62

83.4
4

83.4
7

83.4
25

83.4
46

100

IDF1‚Üë
time [s]

80.6
124

79.9
30

79.9
54

79.9
182

79.9
360

150

IDF1‚Üë
time [s]

78.7
222

76.8
61

76.8
110

76.8
378

76.8
780

200

IDF1‚Üë
time [s]

77.6
354

75.8
95

75.8
177

75.8
604

75.8
1195

8.15. Qualitative Results
Figure 4 and Figure 5 show qualitative tracking results
from the MOT20 [16] and MOT17 [16] datasets. Comparing the samples, it becomes apparent that the density of
objects in MOT20 is much higher than in MOT17. The
sequence MOT20-04 (Figure 4) has an average density of
178.6 objects per frame and sequence MOT17-12 (Figure 5) only 9.6. The high density in MOT20 results in very
crowded groups of persons which are occluding each other
completely or partially. Accordingly, appearance information are ambiguous, leading to less discriminative edge
costs. An additional challenge arises due to the distance
between the camera and the persons, as well as global illumination changes in some sequences. The images shown
in Figure 4 are captured in a temporal distance of 40 frames
(i.e. 1.6 seconds) and the illumination changes heavily. This
leads to appearance changes within a short time, which
makes re-identification challenging. For instance, the person with id 666 (top right corner) in Figure 4 is wearing a
red scarf and a beige jacket. Only a few frames later, the
person is barely visible and colors have changed.
Despite these challenges, our system delivers accurate
tracking results, as can be seen from the result images. Also
the combinatorial and computational challenge in computing optimal trajectories for MOT20, considering for each
detections all possible connections within a 50 frame range
becomes apparent.
Result video for all test sequences can be obtain from the
official evaluation server, for MOT155 , MOT166 , MOT177 ,
and MOT208 .
5 https://motchallenge.net/method/MOT=4031&chl=2
6 https://motchallenge.net/method/MOT=4031&chl=5
7 https://motchallenge.net/method/MOT=4031&chl=10
8 https://motchallenge.net/method/MOT=4031&chl=13

28

Table 6. Runtime and IDF1 comparison of LDP solvers: ApLift (ours) with 6, 11, 31, 51 and 101 iterations and globally optimal (one step)
LifT[28] on MOT17 train. Numbers in parenthesis in the time column show the difference between the solvers, in the IDF1 column the
ratio between Lift and ApLift.
LifT
Time‚Üì IDF1‚Üë

Ours-6
Time‚Üì
IDF1‚Üë

Ours-11
Time‚Üì
IDF1‚Üë

Ours-31
Time‚Üì
IDF1‚Üë

Ours-51
Time‚Üì
IDF1‚Üë

Ours-101
Time‚Üì
IDF1‚Üë

02-DPM

7324
(1.0)

49.4
(0.0)

94
(0.01)

47.4
(‚àí2.00)

157
(0.02)

47.4
(‚àí2.00)

513
(0.07)

49.1
(‚àí0.30)

989
(0.14)

49.1
(‚àí0.30)

2415
(0.33)

49.1
(‚àí0.30)

02-FRCNN

4073
(1.0)

54.7
(0.0)

97
(0.02)

54.9
(0.20)

161
(0.04)

54.9
(0.20)

526
(0.13)

54.9
(0.20)

1021
(0.25)

54.9
(0.20)

2503
(0.61)

54.9
(0.20)

02-SDP

7795
(1.0)

56.7
(0.0)

131
(0.02)

55.0
(‚àí1.70)

219
(0.03)

55.0
(‚àí1.70)

717
(0.09)

55.0
(‚àí1.70)

1410
(0.18)

55.0
(‚àí1.70)

3685
(0.47)

55.0
(‚àí1.70)

04-DPM

‚àó
(‚àó)

75.4
(0.0)

449
(‚àó)

74.7
(‚àí0.70)

756
(‚àó)

74.7
(‚àí0.70)

2220
(‚àó)

74.7
(‚àí0.70)

3929
(‚àó)

75.0
(‚àí0.40)

8578
(‚àó)

75.0
(‚àí0.40)

4889
(1.0)

79.2
(0.0)

383
(0.08)

78.1
(‚àí1.10)

644
(0.13)

78.1
(‚àí1.10)

1811
(0.37)

76.3
(‚àí2.90)

3111
(0.64)

78.2
(‚àí1.00)

6565
(1.34)

78.2
(‚àí1.00)

04-SDP

‚àó
(‚àó)

82.3
(0.0)

499
(‚àó)

78.0
(‚àí4.30)

839
(‚àó)

78.0
(‚àí4.30)

2441
(‚àó)

78.0
(‚àí4.30)

4294
(‚àó)

77.7
(‚àí4.60)

9269
(‚àó)

79.9
(‚àí2.40)

05-DPM

535
(1.0)

65.0
(0.0)

10
(0.02)

62.6
(‚àí2.40)

15
(0.03)

62.6
(‚àí2.40)

57
(0.11)

63.5
(‚àí1.50)

116
(0.22)

63.5
(‚àí1.50)

298
(0.56)

63.5
(‚àí1.50)

05-FRCNN

514
(1.0)

66.6
(0.0)

10
(0.02)

63.8
(‚àí2.80)

15
(0.03)

63.8
(‚àí2.80)

57
(0.11)

64.0
(‚àí2.60)

118
(0.23)

63.9
(‚àí2.70)

315
(0.61)

65.6
(‚àí1.00)

05-SDP

604
(1.0)

67.9
(0.0)

11
(0.02)

67.9
(0.00)

18
(0.03)

67.9
(0.00)

67
(0.11)

67.1
(‚àí0.80)

137
(0.23)

67.1
(‚àí0.80)

364
(0.60)

67.6
(‚àí0.30)

09-DPM

6692
(1.0)

67.5
(0.0)

42
(0.01)

66.4
(‚àí1.10)

70
(0.01)

66.4
(‚àí1.10)

232
(0.03)

67.5
(0.00)

480
(0.07)

67.5
(0.00)

1281
(0.19)

67.5
(0.00)

09-FRCNN

11888
(1.0)

68.2
(0.0)

37
(0.00)

68.2
(0.00)

61
(0.01)

68.2
(0.00)

201
(0.02)

68.2
(0.00)

407
(0.03)

68.2
(0.00)

1095
(0.09)

68.2
(0.00)

09-SDP

1462
(1.0)

68.6
(0.0)

44
(0.03)

67.1
(‚àí1.50)

74
(0.05)

67.1
(‚àí1.50)

247
(0.17)

68.5
(‚àí0.10)

512
(0.35)

68.5
(‚àí0.10)

1443
(0.99)

68.5
(‚àí0.10)

10-DPM

‚àó
(‚àó)

66.0
(0.0)

279
(‚àó)

68.0
(2.00)

466
(‚àó)

68.0
(2.00)

1524
(‚àó)

66.8
(0.80)

3087
(‚àó)

67.0
(1.00)

9478
(‚àó)

67.9
(1.90)

10-FRCNN

‚àó
(‚àó)

65.2
(0.0)

310
(‚àó)

68.8
(3.60)

511
(‚àó)

68.5
(3.30)

1689
(‚àó)

69.4
(4.20)

3428
(‚àó)

69.4
(4.20)

10743
(‚àó)

69.4
(4.20)

10-SDP

‚àó
(‚àó)

65.4
(0.0)

379
(‚àó)

67.0
(1.60)

630
(‚àó)

67.0
(1.60)

2090
(‚àó)

67.4
(2.00)

4294
(‚àó)

67.1
(1.70)

13379
(‚àó)

69.8
(4.40)

11-DPM

1991
(1.0)

76.3
(0.0)

60
(0.03)

76.3
(0.00)

99
(0.05)

76.3
(0.00)

335
(0.17)

76.3
(0.00)

672
(0.34)

76.3
(0.00)

1672
(0.84)

76.3
(0.00)

11-FRCNN

2382
(1.0)

78.3
(0.0)

68
(0.03)

78.3
(0.00)

113
(0.05)

78.3
(0.00)

366
(0.15)

78.3
(0.00)

729
(0.31)

78.3
(0.00)

1799
(0.76)

78.3
(0.00)

11-SDP

3195
(1.0)

80.0
(0.0)

68
(0.02)

79.8
(‚àí0.20)

113
(0.04)

79.8
(‚àí0.20)

370
(0.12)

80.1
(0.10)

748
(0.23)

80.0
(0.00)

2057
(0.64)

80.0
(0.00)

13-DPM

‚àó
(‚àó)

62.8
(0.0)

152
(‚àó)

66.8
(4.00)

252
(‚àó)

66.8
(4.00)

944
(‚àó)

66.8
(4.00)

2008
(‚àó)

66.8
(4.00)

6340
(‚àó)

65.7
(2.90)

13-FRCNN

‚àó
(‚àó)

62.5
(0.0)

217
(‚àó)

69.8
(7.30)

351
(‚àó)

69.8
(7.30)

1331
(‚àó)

69.8
(7.30)

2942
(‚àó)

67.7
(5.20)

9813
(‚àó)

66.2
(3.70)

13-SDP

‚àó
(‚àó)

64.5
(0.0)

196
(‚àó)

66.8
(2.30)

326
(‚àó)

66.8
(2.30)

1237
(‚àó)

66.8
(2.30)

2698
(‚àó)

66.2
(1.70)

8954
(‚àó)

65.6
(1.10)

OVERALL

‚àó
(‚àó)

70.7
(0.0)

168
(‚àó)

70.3
(‚àí0.40)

280
(‚àó)

70.3
(‚àí0.40)

904
(‚àó)

70.2
(‚àí0.50)

1768
(‚àó)

70.3
(‚àí0.40)

4859
(‚àó)

70.7
(0.00)

Sequence Name

04-FRCNN

29

MOTA‚Üë

IDF1‚Üë

MT‚Üë

ML‚Üì

FP‚Üì

FN‚Üì

IDS‚Üì

Frag. ‚Üì

MOT20 Train

MOT20-01
MOT20-02
MOT20-03
MOT20-05
OVERALL

65.8
62.3
80.4
74.6
74.4

62.0
55.1
76.1
57.8
62.8

31
108
427
643
1209

10
18
66
115
209

180
1393
5427
8778
15778

6512
56420
55552
152927
271411

109
548
623
2231
3511

87
534
591
2063
3275

MOT20 Test

Table 7. Evaluation results for training and test sequences for datasets MOT17 [45] and MOT20 [16]

MOT20-04
MOT20-06
MOT20-07
MOT20-08
OVERALL

79.3
36.1
56.9
26.5
58.9

68.8
36.8
54.7
33.8
56.5

412
41
40
20
513

40
111
15
98
264

8315
4786
936
3702
17739

47364
79313
13135
52924
192736

968
740
194
339
2241

840
744
195
333
2112

MOT17-02-DPM
MOT17-02-FRCNN
MOT17-02-SDP
MOT17-04-DPM
MOT17-04-FRCNN
MOT17-04-SDP
MOT17-05-DPM
MOT17-05-FRCNN
MOT17-05-SDP
MOT17-09-DPM
MOT17-09-FRCNN
MOT17-09-SDP
MOT17-10-DPM
MOT17-10-FRCNN
MOT17-10-SDP
MOT17-11-DPM
MOT17-11-FRCNN
MOT17-11-SDP
MOT17-13-DPM
MOT17-13-FRCNN
MOT17-13-SDP
OVERALL

42.2
47.3
55.1
70.9
68.0
77.9
60.0
57.8
62.6
73.0
71.5
74.1
65.3
62.8
66.3
69.2
71.5
72.6
64.4
67.8
67.2
66.0

52.5
58.4
60.5
78.9
78.4
80.8
64.5
64.0
67.8
72.8
68.4
72.9
67.4
65.8
66.5
75.7
76.8
78.5
64.8
63.4
63.7
71.4

14
15
17
40
39
47
48
55
59
14
14
14
32
40
43
34
38
42
55
77
72
809

29
21
16
21
21
13
34
32
19
1
1
1
6
2
2
21
18
13
33
8
18
330

125
227
289
340
179
439
475
650
693
46
105
66
847
2121
1967
248
412
547
627
1739
1388
13530

10588
9532
7994
13481
15044
10035
2260
2225
1842
1380
1403
1302
3545
2513
2189
2624
2233
1981
3436
1892
2312
99811

26
27
53
17
5
29
31
46
53
10
10
10
61
139
173
37
47
58
83
120
117
1152

26
30
52
29
13
68
24
41
46
9
9
11
74
114
120
17
15
19
56
76
60
909

MOT17-01-DPM
MOT17-01-FRCNN
MOT17-01-SDP
MOT17-03-DPM
MOT17-03-FRCNN
MOT17-03-SDP
MOT17-06-DPM
MOT17-06-FRCNN
MOT17-06-SDP
MOT17-07-DPM
MOT17-07-FRCNN
MOT17-07-SDP
MOT17-08-DPM
MOT17-08-FRCNN
MOT17-08-SDP
MOT17-12-DPM
MOT17-12-FRCNN
MOT17-12-SDP
MOT17-14-DPM
MOT17-14-FRCNN
MOT17-14-SDP
OVERALL

48.8
47.0
45.2
73.8
72.8
77.7
57.7
57.3
57.2
45.7
45.0
46.6
33.7
31.5
34.5
47.6
47.8
50.0
37.8
33.9
37.0
60.5

54.3
57.5
55.4
73.4
74.7
75.4
61.2
58.4
59.5
52.5
53.1
53.8
44.3
42.1
45.2
61.9
62.3
66.1
51.0
48.4
49.9
65.6

8
9
9
85
74
94
94
102
107
11
11
13
17
17
18
23
18
19
19
25
25
798

10
10
10
17
17
13
76
59
58
15
15
11
37
37
34
36
40
42
71
62
58
728

113
360
488
4360
3471
4676
1142
1652
1700
1062
1345
1622
421
462
445
563
296
488
1147
2369
2427
30609

3181
3050
3033
22905
24883
18482
3765
3279
3251
8038
7862
7310
13533
13948
13339
3959
4219
3836
10191
9636
8970
190670

8
11
13
118
109
139
77
102
87
80
75
87
48
53
63
20
13
11
151
206
238
1709

21
21
29
261
234
386
91
140
125
126
135
166
67
74
85
32
24
31
150
228
246
2672

MOT17 Test

MOT17 Train

Sequence

30

Figure 4. Example images from sequence MOT20-04 at frames 1092 and 1132. The images shows a crowded scene captured by a static
camera. The above image is captured before an illumination change happens. The lower image is captured after an illumination change
happens. The appearance of persons changes consequently to the illumination changes (e.g. ID 666 in the top right corner). The result
video can be found at https://motchallenge.net/method/MOT=4031&chl=13&vidSeq=MOT20-04.

31

Figure 5. Example images from sequence MOT17-12 at frames 21 and 63. The image shows a scene captured by a moving camera. Compared to MOT20, the number of persons in the scene is lower and occlusions are seldom. The result video can be found at
https://motchallenge.net/method/MOT=4031&chl=10&vidSeq=MOT17-12-FRCNN.

32

